<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>anovos.data_report.report_preprocessing API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>anovos.data_report.report_preprocessing</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pyspark
from pyspark.sql import functions as F
from pyspark.sql import types as T
from anovos.data_transformer.transformers import outlier_categories, imputation_MMM, attribute_binning
from anovos.shared.utils import attributeType_segregation
from anovos.data_analyzer.stats_generator import uniqueCount_computation
from pyspark.sql.window import Window
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
import plotly
from plotly.io import write_json
import plotly.express as px
import plotly.graph_objects as go
from plotly.figure_factory import create_distplot
from io import StringIO,BytesIO 
import boto3

global_theme = px.colors.sequential.Plasma
global_theme_r = px.colors.sequential.Plasma_r
global_plot_bg_color = &#39;rgba(0,0,0,0)&#39;
global_paper_bg_color = &#39;rgba(0,0,0,0)&#39;
num_cols = []
cat_cols = []

def ends_with(string, end_str=&#34;/&#34;):
    &#34;&#34;&#34;

    Args:
      string: s3:mw-bucket&#34;
      end_str: return: &#34;s3:mw-bucket/&#34; (Default value = &#34;/&#34;)

    Returns:
      s3:mw-bucket/&#34;

    &#34;&#34;&#34;
    string = str(string)
    if string.endswith(end_str):
        return string
    return string + end_str


def save_stats(spark, idf, master_path, function_name, reread=False,run_type=&#34;local&#34;):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: input dataframe
      master_path: Path to master folder under which all statistics will be saved in a csv file format.
      function_name: Function Name for which statistics need to be saved. file name will be saved as csv
      reread:  (Default value = False)
      run_type:  (Default value = &#34;local&#34;)

    Returns:
      None, dataframe saved &lt;br&gt;\
      run_type: local or emr based on the mode of execution. Default value is kept as local &lt;br&gt;\
      reread: option to reread. Default value is kept as False

    &#34;&#34;&#34;

    if run_type == &#34;local&#34;:

        Path(master_path).mkdir(parents=True, exist_ok=True)
        idf.toPandas().to_csv(ends_with(master_path) + function_name + &#34;.csv&#34;,index=False)

    else:

        bucket_name = master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0]
        path_name = master_path.replace(master_path.split(&#34;//&#34;)[0]+&#34;//&#34;+master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0],&#34;&#34;)[1:]
        s3_resource = boto3.resource(&#34;s3&#34;)
        csv_buffer = BytesIO()
        idf.toPandas().to_csv(csv_buffer,index=False)
        s3_resource.Object(bucket_name, ends_with(path_name) + function_name + &#34;.csv&#34;).put(Body=csv_buffer.getvalue())

    if reread:
        odf = spark.read.csv(ends_with(master_path) + function_name + &#34;.csv&#34;, header=True, inferSchema=True)
        return odf


def edit_binRange(col):
    &#34;&#34;&#34;

    Args:
      col: The column which is passed as input and needs to be treated. The generated output will not contain \
      any range whose value at either side is the same.

    Returns:

    &#34;&#34;&#34;
    try:
        list_col = col.split(&#34;-&#34;)
        deduped_col = list(set(list_col))
        if len(list_col) != len(deduped_col):
            return deduped_col[0]
        else:
            return col
    except:
        pass


f_edit_binRange = F.udf(edit_binRange, T.StringType())


def binRange_to_binIdx(spark, col, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      col: The input column which is needed to by mapped with respective index
      cutoffs_path: paths containing the range cutoffs applicable for each index

    Returns:

    &#34;&#34;&#34;
    bin_cutoffs = spark.read.parquet(cutoffs_path).where(F.col(&#39;attribute&#39;) == col).select(&#39;parameters&#39;) \
        .rdd.flatMap(lambda x: x).collect()[0]
    bin_ranges = []
    max_cat = len(bin_cutoffs) + 1
    for idx in range(0, max_cat):
        if idx == 0:
            bin_ranges.append(&#34;&lt;= &#34; + str(round(bin_cutoffs[idx], 4)))
        elif idx &lt; (max_cat - 1):
            bin_ranges.append(str(round(bin_cutoffs[idx - 1], 4)) + &#34;-&#34; + str(round(bin_cutoffs[idx], 4)))
        else:
            bin_ranges.append(&#34;&gt; &#34; + str(round(bin_cutoffs[idx - 1], 4)))
    mapping = spark.createDataFrame(zip(range(1, max_cat + 1), bin_ranges), schema=[&#34;bin_idx&#34;, col])
    return mapping


def plot_frequency(spark, idf, col, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms
      col: Analysis column
      cutoffs_path: Path containing the range cut offs details for the analysis column

    Returns:

    &#34;&#34;&#34;
    odf = idf.groupBy(col).count() \
        .withColumn(&#34;count_%&#34;, 100 * (F.col(&#34;count&#34;) / F.sum(&#34;count&#34;).over(Window.partitionBy()))) \
        .withColumn(col, f_edit_binRange(col))

    if col in cat_cols:
        odf_pd = odf.orderBy(&#34;count&#34;, ascending=False).toPandas().fillna(&#34;Missing&#34;)
        odf_pd.loc[odf_pd[col] == &#34;others&#34;, col] = &#34;others*&#34;

    if col in num_cols:
        mapping = binRange_to_binIdx(spark, col, cutoffs_path)
        odf_pd = odf.join(mapping, col, &#39;left_outer&#39;).orderBy(&#39;bin_idx&#39;).toPandas().fillna(&#34;Missing&#34;)


    
    fig = px.bar(odf_pd, x=col, y=&#39;count&#39;, text=odf_pd[&#39;count_%&#39;].apply(lambda x: &#39;{0:1.2f}%&#39;.format(x)),
                 color_discrete_sequence=global_theme)
    fig.update_traces(textposition=&#39;outside&#39;)
    fig.update_layout(title_text=str(&#39;Frequency Distribution for &#39; + str(col.upper())))
    fig.update_xaxes(type=&#39;category&#39;)
    # fig.update_layout(barmode=&#39;stack&#39;, xaxis={&#39;categoryorder&#39;:&#39;total descending&#39;})
    fig.layout.plot_bgcolor = global_plot_bg_color
    fig.layout.paper_bgcolor = global_paper_bg_color
    # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f&#34;{base_loc}/{file_name_}bar_graph.html&#34;)

    return fig


def plot_outlier(spark, idf, col, split_var=None, sample_size=500000):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe which would be referred for capturing the outliers in form of violin charts
      col: Analysis column
      split_var: Column which is needed. Default value is kept as None
      sample_size: Maximum Sample size. Default value is kept as 500000

    Returns:

    &#34;&#34;&#34;
    idf_sample = idf.select(col).sample(False, min(1.0, float(sample_size) / idf.count()), 0)
    idf_sample.persist(pyspark.StorageLevel.MEMORY_AND_DISK).count()
    idf_imputed = imputation_MMM(spark, idf_sample)
    idf_pd = idf_imputed.toPandas()
    fig = px.violin(idf_pd, y=col, color=split_var, box=True, points=&#34;outliers&#34;,
                    color_discrete_sequence=[global_theme_r[8], global_theme_r[4]])
    fig.layout.plot_bgcolor = global_plot_bg_color
    fig.layout.paper_bgcolor = global_paper_bg_color
    fig.update_layout(legend=dict(orientation=&#34;h&#34;, x=0.5, yanchor=&#34;bottom&#34;, xanchor=&#34;center&#34;))

    return fig


def plot_eventRate(spark, idf, col, label_col, event_label, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram
      col: Analysis column
      label_col: Label column
      event_label: Event label
      cutoffs_path: Path containing the range cut offs details for the analysis column

    Returns:

    &#34;&#34;&#34;
    event_label = str(event_label)
    class_cats = idf.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()

    odf = idf.groupBy(col).pivot(label_col).count() \
        .fillna(0, subset=class_cats) \
        .withColumn(&#34;event_rate&#34;, 100 * (F.col(event_label) / (F.col(class_cats[0]) + F.col(class_cats[1])))) \
        .withColumn(&#34;attribute_name&#34;, F.lit(col)) \
        .withColumn(col, f_edit_binRange(col))

    if col in cat_cols:
        odf_pd = odf.orderBy(&#34;event_rate&#34;, ascending=False).toPandas()
        odf_pd.loc[odf_pd[col] == &#34;others&#34;, col] = &#34;others*&#34;

    if col in num_cols:
        mapping = binRange_to_binIdx(spark, col, cutoffs_path)
        odf_pd = odf.join(mapping, col, &#39;left_outer&#39;).orderBy(&#39;bin_idx&#39;).toPandas()

    

    fig = px.bar(odf_pd, x=col, y=&#39;event_rate&#39;, text=odf_pd[&#39;event_rate&#39;].apply(lambda x: &#39;{0:1.2f}%&#39;.format(x)),
                 color_discrete_sequence=global_theme)
    fig.update_traces(textposition=&#39;outside&#39;)
    fig.update_layout(title_text=str(
        &#39;Event Rate Distribution for &#39; + str(col.upper()) + str(&#34; [Target Variable : &#34; + str(event_label) + str(&#34;]&#34;))))
    fig.update_xaxes(type=&#39;category&#39;)
    fig.layout.plot_bgcolor = global_plot_bg_color
    fig.layout.paper_bgcolor = global_paper_bg_color
    # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f&#34;{base_loc}/{file_name_}feat_analysis_label.html&#34;)

    return fig


def plot_comparative_drift(spark, idf, source, col, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram
      source: Source dataframe of comparison
      col: Analysis column
      cutoffs_path: Path containing the range cut offs details for the analysis column

    Returns:

    &#34;&#34;&#34;

    odf = idf.groupBy(col).agg((F.count(col) / idf.count()).alias(&#39;countpct_target&#39;)).fillna(np.nan, subset=[col])

    if col in cat_cols:
        odf_pd = odf.join(source.withColumnRenamed(&#34;p&#34;, &#34;countpct_source&#34;).fillna(np.nan, subset=[col]), col,
                          &#34;full_outer&#34;) \
            .orderBy(&#34;countpct_target&#34;, ascending=False).toPandas()

    if col in num_cols:
        mapping = binRange_to_binIdx(spark, col, cutoffs_path)
        odf_pd = odf.join(mapping, col, &#39;left_outer&#39;).fillna(np.nan, subset=[&#39;bin_idx&#39;]) \
            .join(source.fillna(np.nan, subset=[col]).select(F.col(col).alias(&#39;bin_idx&#39;),
                                                             F.col(&#34;p&#34;).alias(&#34;countpct_source&#34;)), &#39;bin_idx&#39;,
                  &#34;full_outer&#34;) \
            .orderBy(&#39;bin_idx&#39;).toPandas()


    odf_pd.fillna({col: &#39;Missing&#39;, &#39;countpct_source&#39;: 0, &#39;countpct_target&#39;: 0}, inplace=True)
    odf_pd[&#39;%_diff&#39;] = (((odf_pd[&#39;countpct_target&#39;] / odf_pd[&#39;countpct_source&#39;]) - 1) * 100)
    fig = go.Figure()
    fig.add_bar(y=list(odf_pd.countpct_source.values), x=odf_pd[col], name=&#34;source&#34;, marker=dict(color=global_theme))
    fig.update_traces(overwrite=True, marker={&#34;opacity&#34;: 0.7})
    fig.add_bar(y=list(odf_pd.countpct_target.values), x=odf_pd[col], name=&#34;target&#34;,
                text=odf_pd[&#39;%_diff&#39;].apply(lambda x: &#39;{0:0.2f}%&#39;.format(x)), marker=dict(color=global_theme))
    fig.update_traces(textposition=&#39;outside&#39;)
    fig.update_layout(paper_bgcolor=global_paper_bg_color, plot_bgcolor=global_plot_bg_color, showlegend=False)
    fig.update_layout(title_text=str(&#39;Drift Comparison for &#39; + col + &#39;&lt;br&gt;&lt;sup&gt;(L-&gt;R : Source-&gt;Target)&lt;/sup&gt;&#39;))
    fig.update_traces(marker=dict(color=global_theme))
    fig.update_xaxes(type=&#39;category&#39;)
    #fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode=&#39;lines+markers&#39;,
    #                        line=dict(color=px.colors.qualitative.Antique[10], width=3, dash=&#39;dot&#39;)))
    fig.update_layout(xaxis_tickfont_size=14, yaxis=dict(title=&#39;frequency&#39;, titlefont_size=16, tickfont_size=14))

    return fig


def charts_to_objects(spark, idf, list_of_cols=&#39;all&#39;, drop_cols=[], label_col=None, event_label=1,
                      bin_method=&#34;equal_range&#34;, bin_size=10, coverage=1.0,
                      drift_detector=False, source_path=&#34;NA&#34;, master_path=&#39;&#39;,run_type=&#34;local&#34;):

    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe
      list_of_cols: List of columns passed for analysis (Default value = &#39;all&#39;)
      drop_cols: List of columns dropped from analysis (Default value = [])
      label_col: Label column (Default value = None)
      event_label: Event label (Default value = 1)
      bin_method: Binning method equal_range or equal_frequency (Default value = &#34;equal_range&#34;)
      bin_size: Maximum bin size categories. Default value is kept as 10
      coverage: Maximum coverage of categories. Default value is kept as 1.0 (which is 100%)
      drift_detector: True or False as per the availability. Default value is kept as False
      source_path: Source data path. Default value is kept as &#34;NA&#34;
      master_path: Path where the output needs to be saved, ideally the same path where the analyzed data output &lt;br&gt;\
      is also saved (Default value = &#39;&#39;)
      run_type: local or emr run type. Default value is kept as local

    Returns:

    &#34;&#34;&#34;
    
    global num_cols
    global cat_cols
    import timeit
    
    
    start = timeit.default_timer()
    
    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]
    
    list_of_cols = list(set([e for e in list_of_cols if e not in drop_cols]))

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    
    num_cols,cat_cols,other_cols = attributeType_segregation(idf.select(list_of_cols))

    idf_cleaned = outlier_categories(spark, idf, list_of_cols=cat_cols, coverage=coverage, max_category=bin_size)
    
    if drift_detector:
        encoding_model_exists = True
    else:
        encoding_model_exists = False
    idf_encoded = attribute_binning(spark, idf_cleaned, list_of_cols=num_cols, method_type=bin_method, bin_size=bin_size, 
                                    bin_dtype=&#34;categorical&#34;, pre_existing_model=encoding_model_exists, 
                                    model_path=source_path+&#34;/drift_statistics&#34;, output_mode=&#39;append&#39;)

    cutoffs_path = source_path+&#34;/drift_statistics/attribute_binning&#34;
    idf_encoded.persist(pyspark.StorageLevel.MEMORY_AND_DISK)
    
    
    if run_type == &#34;local&#34;:

        Path(master_path).mkdir(parents=True, exist_ok=True)
        for idx, col in enumerate(list_of_cols):
            
            if col in cat_cols:
                f = plot_frequency(spark, idf_encoded,col,cutoffs_path)
                f.write_json(ends_with(master_path) + &#34;freqDist_&#34; + col)

                if label_col:
                    f = plot_eventRate(spark,idf_encoded,col,label_col,event_label,cutoffs_path)
                    f.write_json(ends_with(master_path) + &#34;eventDist_&#34; + col)

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded,idf_source,col,cutoffs_path)
                        f.write_json(ends_with(master_path) + &#34;drift_&#34; + col)
                    except:
                        pass
            
            if col in num_cols:
                f = plot_outlier(spark,idf,col,split_var=None)
                f.write_json(ends_with(master_path) + &#34;outlier_&#34; + col)
                f = plot_frequency(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,cutoffs_path)
                f.write_json(ends_with(master_path) + &#34;freqDist_&#34; + col)

                if label_col:
                    f = plot_eventRate(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,label_col,event_label,cutoffs_path)
                    f.write_json(ends_with(master_path) + &#34;eventDist_&#34; + col)

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),idf_source,col,cutoffs_path)
                        f.write_json(ends_with(master_path) + &#34;drift_&#34; + col)
                    except:
                        pass

        pd.DataFrame(idf.dtypes,columns=[&#34;attribute&#34;,&#34;data_type&#34;]).to_csv(ends_with(master_path) + &#34;data_type.csv&#34;,index=False)

    else:

        bucket_name = master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0]
        path_name = master_path.replace(master_path.split(&#34;//&#34;)[0]+&#34;//&#34;+master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0],&#34;&#34;)[1:]
        s3_resource = boto3.resource(&#34;s3&#34;)

        x = pd.DataFrame(idf.dtypes).reset_index()
        x = x.rename(columns = {&#39;index&#39; : &#39;attribute&#39;, 0:&#39;data_type&#39;})
        csv_buffer = BytesIO()
        x.to_csv(csv_buffer,index=False)
        s3_resource.Object(bucket_name, ends_with(path_name) + &#34;data_type.csv&#34;).put(Body=csv_buffer.getvalue())

        for idx, col in enumerate(list_of_cols):
            
            if col in cat_cols:
                f = plot_frequency(spark,idf_encoded,col,cutoffs_path)
                s3_resource.Object(bucket_name, ends_with(path_name) + &#34;freqDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if label_col:
                    f = plot_eventRate(spark,idf_encoded,col,label_col,event_label,cutoffs_path)
                    s3_resource.Object(bucket_name, ends_with(path_name) + &#34;eventDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded,idf_source,col,cutoffs_path)
                        s3_resource.Object(bucket_name, ends_with(path_name) + &#34;drift_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))
                    except:
                        pass
            
            if col in num_cols:
                f = plot_outlier(spark,idf,col,split_var=None)
                s3_resource.Object(bucket_name, ends_with(path_name) + &#34;outlier_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))
                f = plot_frequency(spark, idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,cutoffs_path)
                s3_resource.Object(bucket_name, ends_with(path_name) + &#34;freqDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if label_col:
                    f = plot_eventRate(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,label_col,event_label,cutoffs_path)
                    s3_resource.Object(bucket_name, ends_with(path_name) + &#34;eventDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),idf_source,col,cutoffs_path)
                        s3_resource.Object(bucket_name, ends_with(path_name) + &#34;drift_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))
                    except:
                        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="anovos.data_report.report_preprocessing.binRange_to_binIdx"><code class="name flex">
<span>def <span class="ident">binRange_to_binIdx</span></span>(<span>spark, col, cutoffs_path)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>col</code></strong></dt>
<dd>The input column which is needed to by mapped with respective index</dd>
<dt><strong><code>cutoffs_path</code></strong></dt>
<dd>paths containing the range cutoffs applicable for each index</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binRange_to_binIdx(spark, col, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      col: The input column which is needed to by mapped with respective index
      cutoffs_path: paths containing the range cutoffs applicable for each index

    Returns:

    &#34;&#34;&#34;
    bin_cutoffs = spark.read.parquet(cutoffs_path).where(F.col(&#39;attribute&#39;) == col).select(&#39;parameters&#39;) \
        .rdd.flatMap(lambda x: x).collect()[0]
    bin_ranges = []
    max_cat = len(bin_cutoffs) + 1
    for idx in range(0, max_cat):
        if idx == 0:
            bin_ranges.append(&#34;&lt;= &#34; + str(round(bin_cutoffs[idx], 4)))
        elif idx &lt; (max_cat - 1):
            bin_ranges.append(str(round(bin_cutoffs[idx - 1], 4)) + &#34;-&#34; + str(round(bin_cutoffs[idx], 4)))
        else:
            bin_ranges.append(&#34;&gt; &#34; + str(round(bin_cutoffs[idx - 1], 4)))
    mapping = spark.createDataFrame(zip(range(1, max_cat + 1), bin_ranges), schema=[&#34;bin_idx&#34;, col])
    return mapping</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.charts_to_objects"><code class="name flex">
<span>def <span class="ident">charts_to_objects</span></span>(<span>spark, idf, list_of_cols='all', drop_cols=[], label_col=None, event_label=1, bin_method='equal_range', bin_size=10, coverage=1.0, drift_detector=False, source_path='NA', master_path='', run_type='local')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>idf</code></strong></dt>
<dd>Input dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>List of columns passed for analysis (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns dropped from analysis (Default value = [])</dd>
<dt><strong><code>label_col</code></strong></dt>
<dd>Label column (Default value = None)</dd>
<dt><strong><code>event_label</code></strong></dt>
<dd>Event label (Default value = 1)</dd>
<dt><strong><code>bin_method</code></strong></dt>
<dd>Binning method equal_range or equal_frequency (Default value = "equal_range")</dd>
<dt><strong><code>bin_size</code></strong></dt>
<dd>Maximum bin size categories. Default value is kept as 10</dd>
<dt><strong><code>coverage</code></strong></dt>
<dd>Maximum coverage of categories. Default value is kept as 1.0 (which is 100%)</dd>
<dt><strong><code>drift_detector</code></strong></dt>
<dd>True or False as per the availability. Default value is kept as False</dd>
<dt><strong><code>source_path</code></strong></dt>
<dd>Source data path. Default value is kept as "NA"</dd>
<dt><strong><code>master_path</code></strong></dt>
<dd>Path where the output needs to be saved, ideally the same path where the analyzed data output <br>
is also saved (Default value = '')</dd>
<dt><strong><code>run_type</code></strong></dt>
<dd>local or emr run type. Default value is kept as local</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def charts_to_objects(spark, idf, list_of_cols=&#39;all&#39;, drop_cols=[], label_col=None, event_label=1,
                      bin_method=&#34;equal_range&#34;, bin_size=10, coverage=1.0,
                      drift_detector=False, source_path=&#34;NA&#34;, master_path=&#39;&#39;,run_type=&#34;local&#34;):

    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe
      list_of_cols: List of columns passed for analysis (Default value = &#39;all&#39;)
      drop_cols: List of columns dropped from analysis (Default value = [])
      label_col: Label column (Default value = None)
      event_label: Event label (Default value = 1)
      bin_method: Binning method equal_range or equal_frequency (Default value = &#34;equal_range&#34;)
      bin_size: Maximum bin size categories. Default value is kept as 10
      coverage: Maximum coverage of categories. Default value is kept as 1.0 (which is 100%)
      drift_detector: True or False as per the availability. Default value is kept as False
      source_path: Source data path. Default value is kept as &#34;NA&#34;
      master_path: Path where the output needs to be saved, ideally the same path where the analyzed data output &lt;br&gt;\
      is also saved (Default value = &#39;&#39;)
      run_type: local or emr run type. Default value is kept as local

    Returns:

    &#34;&#34;&#34;
    
    global num_cols
    global cat_cols
    import timeit
    
    
    start = timeit.default_timer()
    
    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]
    
    list_of_cols = list(set([e for e in list_of_cols if e not in drop_cols]))

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    
    num_cols,cat_cols,other_cols = attributeType_segregation(idf.select(list_of_cols))

    idf_cleaned = outlier_categories(spark, idf, list_of_cols=cat_cols, coverage=coverage, max_category=bin_size)
    
    if drift_detector:
        encoding_model_exists = True
    else:
        encoding_model_exists = False
    idf_encoded = attribute_binning(spark, idf_cleaned, list_of_cols=num_cols, method_type=bin_method, bin_size=bin_size, 
                                    bin_dtype=&#34;categorical&#34;, pre_existing_model=encoding_model_exists, 
                                    model_path=source_path+&#34;/drift_statistics&#34;, output_mode=&#39;append&#39;)

    cutoffs_path = source_path+&#34;/drift_statistics/attribute_binning&#34;
    idf_encoded.persist(pyspark.StorageLevel.MEMORY_AND_DISK)
    
    
    if run_type == &#34;local&#34;:

        Path(master_path).mkdir(parents=True, exist_ok=True)
        for idx, col in enumerate(list_of_cols):
            
            if col in cat_cols:
                f = plot_frequency(spark, idf_encoded,col,cutoffs_path)
                f.write_json(ends_with(master_path) + &#34;freqDist_&#34; + col)

                if label_col:
                    f = plot_eventRate(spark,idf_encoded,col,label_col,event_label,cutoffs_path)
                    f.write_json(ends_with(master_path) + &#34;eventDist_&#34; + col)

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded,idf_source,col,cutoffs_path)
                        f.write_json(ends_with(master_path) + &#34;drift_&#34; + col)
                    except:
                        pass
            
            if col in num_cols:
                f = plot_outlier(spark,idf,col,split_var=None)
                f.write_json(ends_with(master_path) + &#34;outlier_&#34; + col)
                f = plot_frequency(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,cutoffs_path)
                f.write_json(ends_with(master_path) + &#34;freqDist_&#34; + col)

                if label_col:
                    f = plot_eventRate(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,label_col,event_label,cutoffs_path)
                    f.write_json(ends_with(master_path) + &#34;eventDist_&#34; + col)

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),idf_source,col,cutoffs_path)
                        f.write_json(ends_with(master_path) + &#34;drift_&#34; + col)
                    except:
                        pass

        pd.DataFrame(idf.dtypes,columns=[&#34;attribute&#34;,&#34;data_type&#34;]).to_csv(ends_with(master_path) + &#34;data_type.csv&#34;,index=False)

    else:

        bucket_name = master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0]
        path_name = master_path.replace(master_path.split(&#34;//&#34;)[0]+&#34;//&#34;+master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0],&#34;&#34;)[1:]
        s3_resource = boto3.resource(&#34;s3&#34;)

        x = pd.DataFrame(idf.dtypes).reset_index()
        x = x.rename(columns = {&#39;index&#39; : &#39;attribute&#39;, 0:&#39;data_type&#39;})
        csv_buffer = BytesIO()
        x.to_csv(csv_buffer,index=False)
        s3_resource.Object(bucket_name, ends_with(path_name) + &#34;data_type.csv&#34;).put(Body=csv_buffer.getvalue())

        for idx, col in enumerate(list_of_cols):
            
            if col in cat_cols:
                f = plot_frequency(spark,idf_encoded,col,cutoffs_path)
                s3_resource.Object(bucket_name, ends_with(path_name) + &#34;freqDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if label_col:
                    f = plot_eventRate(spark,idf_encoded,col,label_col,event_label,cutoffs_path)
                    s3_resource.Object(bucket_name, ends_with(path_name) + &#34;eventDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded,idf_source,col,cutoffs_path)
                        s3_resource.Object(bucket_name, ends_with(path_name) + &#34;drift_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))
                    except:
                        pass
            
            if col in num_cols:
                f = plot_outlier(spark,idf,col,split_var=None)
                s3_resource.Object(bucket_name, ends_with(path_name) + &#34;outlier_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))
                f = plot_frequency(spark, idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,cutoffs_path)
                s3_resource.Object(bucket_name, ends_with(path_name) + &#34;freqDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if label_col:
                    f = plot_eventRate(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),col,label_col,event_label,cutoffs_path)
                    s3_resource.Object(bucket_name, ends_with(path_name) + &#34;eventDist_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))

                if drift_detector:
                    try:
                        frequency_path = source_path+&#34;/drift_statistics/frequency_counts/&#34; + col
                        idf_source = spark.read.csv(frequency_path, header=True, inferSchema=True)
                        f = plot_comparative_drift(spark,idf_encoded.drop(col).withColumnRenamed(col+&#34;_binned&#34;,col),idf_source,col,cutoffs_path)
                        s3_resource.Object(bucket_name, ends_with(path_name) + &#34;drift_&#34; + col).put(Body=(bytes(json.dumps(f.to_json()).encode(&#34;UTF-8&#34;))))
                    except:
                        pass</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.edit_binRange"><code class="name flex">
<span>def <span class="ident">edit_binRange</span></span>(<span>col)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>col</code></strong></dt>
<dd>The column which is passed as input and needs to be treated. The generated output will not contain
any range whose value at either side is the same.</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_binRange(col):
    &#34;&#34;&#34;

    Args:
      col: The column which is passed as input and needs to be treated. The generated output will not contain \
      any range whose value at either side is the same.

    Returns:

    &#34;&#34;&#34;
    try:
        list_col = col.split(&#34;-&#34;)
        deduped_col = list(set(list_col))
        if len(list_col) != len(deduped_col):
            return deduped_col[0]
        else:
            return col
    except:
        pass</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.ends_with"><code class="name flex">
<span>def <span class="ident">ends_with</span></span>(<span>string, end_str='/')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>string</code></strong></dt>
<dd>s3:mw-bucket"</dd>
<dt><strong><code>end_str</code></strong></dt>
<dd>return: "s3:mw-bucket/" (Default value = "/")</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>s3:mw-bucket/"</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ends_with(string, end_str=&#34;/&#34;):
    &#34;&#34;&#34;

    Args:
      string: s3:mw-bucket&#34;
      end_str: return: &#34;s3:mw-bucket/&#34; (Default value = &#34;/&#34;)

    Returns:
      s3:mw-bucket/&#34;

    &#34;&#34;&#34;
    string = str(string)
    if string.endswith(end_str):
        return string
    return string + end_str</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.f_edit_binRange"><code class="name flex">
<span>def <span class="ident">f_edit_binRange</span></span>(<span>col)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>col</code></strong></dt>
<dd>The column which is passed as input and needs to be treated. The generated output will not contain
any range whose value at either side is the same.</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_binRange(col):
    &#34;&#34;&#34;

    Args:
      col: The column which is passed as input and needs to be treated. The generated output will not contain \
      any range whose value at either side is the same.

    Returns:

    &#34;&#34;&#34;
    try:
        list_col = col.split(&#34;-&#34;)
        deduped_col = list(set(list_col))
        if len(list_col) != len(deduped_col):
            return deduped_col[0]
        else:
            return col
    except:
        pass</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.plot_comparative_drift"><code class="name flex">
<span>def <span class="ident">plot_comparative_drift</span></span>(<span>spark, idf, source, col, cutoffs_path)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>idf</code></strong></dt>
<dd>Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram</dd>
<dt><strong><code>source</code></strong></dt>
<dd>Source dataframe of comparison</dd>
<dt><strong><code>col</code></strong></dt>
<dd>Analysis column</dd>
<dt><strong><code>cutoffs_path</code></strong></dt>
<dd>Path containing the range cut offs details for the analysis column</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_comparative_drift(spark, idf, source, col, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram
      source: Source dataframe of comparison
      col: Analysis column
      cutoffs_path: Path containing the range cut offs details for the analysis column

    Returns:

    &#34;&#34;&#34;

    odf = idf.groupBy(col).agg((F.count(col) / idf.count()).alias(&#39;countpct_target&#39;)).fillna(np.nan, subset=[col])

    if col in cat_cols:
        odf_pd = odf.join(source.withColumnRenamed(&#34;p&#34;, &#34;countpct_source&#34;).fillna(np.nan, subset=[col]), col,
                          &#34;full_outer&#34;) \
            .orderBy(&#34;countpct_target&#34;, ascending=False).toPandas()

    if col in num_cols:
        mapping = binRange_to_binIdx(spark, col, cutoffs_path)
        odf_pd = odf.join(mapping, col, &#39;left_outer&#39;).fillna(np.nan, subset=[&#39;bin_idx&#39;]) \
            .join(source.fillna(np.nan, subset=[col]).select(F.col(col).alias(&#39;bin_idx&#39;),
                                                             F.col(&#34;p&#34;).alias(&#34;countpct_source&#34;)), &#39;bin_idx&#39;,
                  &#34;full_outer&#34;) \
            .orderBy(&#39;bin_idx&#39;).toPandas()


    odf_pd.fillna({col: &#39;Missing&#39;, &#39;countpct_source&#39;: 0, &#39;countpct_target&#39;: 0}, inplace=True)
    odf_pd[&#39;%_diff&#39;] = (((odf_pd[&#39;countpct_target&#39;] / odf_pd[&#39;countpct_source&#39;]) - 1) * 100)
    fig = go.Figure()
    fig.add_bar(y=list(odf_pd.countpct_source.values), x=odf_pd[col], name=&#34;source&#34;, marker=dict(color=global_theme))
    fig.update_traces(overwrite=True, marker={&#34;opacity&#34;: 0.7})
    fig.add_bar(y=list(odf_pd.countpct_target.values), x=odf_pd[col], name=&#34;target&#34;,
                text=odf_pd[&#39;%_diff&#39;].apply(lambda x: &#39;{0:0.2f}%&#39;.format(x)), marker=dict(color=global_theme))
    fig.update_traces(textposition=&#39;outside&#39;)
    fig.update_layout(paper_bgcolor=global_paper_bg_color, plot_bgcolor=global_plot_bg_color, showlegend=False)
    fig.update_layout(title_text=str(&#39;Drift Comparison for &#39; + col + &#39;&lt;br&gt;&lt;sup&gt;(L-&gt;R : Source-&gt;Target)&lt;/sup&gt;&#39;))
    fig.update_traces(marker=dict(color=global_theme))
    fig.update_xaxes(type=&#39;category&#39;)
    #fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode=&#39;lines+markers&#39;,
    #                        line=dict(color=px.colors.qualitative.Antique[10], width=3, dash=&#39;dot&#39;)))
    fig.update_layout(xaxis_tickfont_size=14, yaxis=dict(title=&#39;frequency&#39;, titlefont_size=16, tickfont_size=14))

    return fig</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.plot_eventRate"><code class="name flex">
<span>def <span class="ident">plot_eventRate</span></span>(<span>spark, idf, col, label_col, event_label, cutoffs_path)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>idf</code></strong></dt>
<dd>Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram</dd>
<dt><strong><code>col</code></strong></dt>
<dd>Analysis column</dd>
<dt><strong><code>label_col</code></strong></dt>
<dd>Label column</dd>
<dt><strong><code>event_label</code></strong></dt>
<dd>Event label</dd>
<dt><strong><code>cutoffs_path</code></strong></dt>
<dd>Path containing the range cut offs details for the analysis column</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_eventRate(spark, idf, col, label_col, event_label, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram
      col: Analysis column
      label_col: Label column
      event_label: Event label
      cutoffs_path: Path containing the range cut offs details for the analysis column

    Returns:

    &#34;&#34;&#34;
    event_label = str(event_label)
    class_cats = idf.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()

    odf = idf.groupBy(col).pivot(label_col).count() \
        .fillna(0, subset=class_cats) \
        .withColumn(&#34;event_rate&#34;, 100 * (F.col(event_label) / (F.col(class_cats[0]) + F.col(class_cats[1])))) \
        .withColumn(&#34;attribute_name&#34;, F.lit(col)) \
        .withColumn(col, f_edit_binRange(col))

    if col in cat_cols:
        odf_pd = odf.orderBy(&#34;event_rate&#34;, ascending=False).toPandas()
        odf_pd.loc[odf_pd[col] == &#34;others&#34;, col] = &#34;others*&#34;

    if col in num_cols:
        mapping = binRange_to_binIdx(spark, col, cutoffs_path)
        odf_pd = odf.join(mapping, col, &#39;left_outer&#39;).orderBy(&#39;bin_idx&#39;).toPandas()

    

    fig = px.bar(odf_pd, x=col, y=&#39;event_rate&#39;, text=odf_pd[&#39;event_rate&#39;].apply(lambda x: &#39;{0:1.2f}%&#39;.format(x)),
                 color_discrete_sequence=global_theme)
    fig.update_traces(textposition=&#39;outside&#39;)
    fig.update_layout(title_text=str(
        &#39;Event Rate Distribution for &#39; + str(col.upper()) + str(&#34; [Target Variable : &#34; + str(event_label) + str(&#34;]&#34;))))
    fig.update_xaxes(type=&#39;category&#39;)
    fig.layout.plot_bgcolor = global_plot_bg_color
    fig.layout.paper_bgcolor = global_paper_bg_color
    # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f&#34;{base_loc}/{file_name_}feat_analysis_label.html&#34;)

    return fig</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.plot_frequency"><code class="name flex">
<span>def <span class="ident">plot_frequency</span></span>(<span>spark, idf, col, cutoffs_path)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>idf</code></strong></dt>
<dd>Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms</dd>
<dt><strong><code>col</code></strong></dt>
<dd>Analysis column</dd>
<dt><strong><code>cutoffs_path</code></strong></dt>
<dd>Path containing the range cut offs details for the analysis column</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_frequency(spark, idf, col, cutoffs_path):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms
      col: Analysis column
      cutoffs_path: Path containing the range cut offs details for the analysis column

    Returns:

    &#34;&#34;&#34;
    odf = idf.groupBy(col).count() \
        .withColumn(&#34;count_%&#34;, 100 * (F.col(&#34;count&#34;) / F.sum(&#34;count&#34;).over(Window.partitionBy()))) \
        .withColumn(col, f_edit_binRange(col))

    if col in cat_cols:
        odf_pd = odf.orderBy(&#34;count&#34;, ascending=False).toPandas().fillna(&#34;Missing&#34;)
        odf_pd.loc[odf_pd[col] == &#34;others&#34;, col] = &#34;others*&#34;

    if col in num_cols:
        mapping = binRange_to_binIdx(spark, col, cutoffs_path)
        odf_pd = odf.join(mapping, col, &#39;left_outer&#39;).orderBy(&#39;bin_idx&#39;).toPandas().fillna(&#34;Missing&#34;)


    
    fig = px.bar(odf_pd, x=col, y=&#39;count&#39;, text=odf_pd[&#39;count_%&#39;].apply(lambda x: &#39;{0:1.2f}%&#39;.format(x)),
                 color_discrete_sequence=global_theme)
    fig.update_traces(textposition=&#39;outside&#39;)
    fig.update_layout(title_text=str(&#39;Frequency Distribution for &#39; + str(col.upper())))
    fig.update_xaxes(type=&#39;category&#39;)
    # fig.update_layout(barmode=&#39;stack&#39;, xaxis={&#39;categoryorder&#39;:&#39;total descending&#39;})
    fig.layout.plot_bgcolor = global_plot_bg_color
    fig.layout.paper_bgcolor = global_paper_bg_color
    # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f&#34;{base_loc}/{file_name_}bar_graph.html&#34;)

    return fig</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.plot_outlier"><code class="name flex">
<span>def <span class="ident">plot_outlier</span></span>(<span>spark, idf, col, split_var=None, sample_size=500000)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>idf</code></strong></dt>
<dd>Input dataframe which would be referred for capturing the outliers in form of violin charts</dd>
<dt><strong><code>col</code></strong></dt>
<dd>Analysis column</dd>
<dt><strong><code>split_var</code></strong></dt>
<dd>Column which is needed. Default value is kept as None</dd>
<dt><strong><code>sample_size</code></strong></dt>
<dd>Maximum Sample size. Default value is kept as 500000</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_outlier(spark, idf, col, split_var=None, sample_size=500000):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: Input dataframe which would be referred for capturing the outliers in form of violin charts
      col: Analysis column
      split_var: Column which is needed. Default value is kept as None
      sample_size: Maximum Sample size. Default value is kept as 500000

    Returns:

    &#34;&#34;&#34;
    idf_sample = idf.select(col).sample(False, min(1.0, float(sample_size) / idf.count()), 0)
    idf_sample.persist(pyspark.StorageLevel.MEMORY_AND_DISK).count()
    idf_imputed = imputation_MMM(spark, idf_sample)
    idf_pd = idf_imputed.toPandas()
    fig = px.violin(idf_pd, y=col, color=split_var, box=True, points=&#34;outliers&#34;,
                    color_discrete_sequence=[global_theme_r[8], global_theme_r[4]])
    fig.layout.plot_bgcolor = global_plot_bg_color
    fig.layout.paper_bgcolor = global_paper_bg_color
    fig.update_layout(legend=dict(orientation=&#34;h&#34;, x=0.5, yanchor=&#34;bottom&#34;, xanchor=&#34;center&#34;))

    return fig</code></pre>
</details>
</dd>
<dt id="anovos.data_report.report_preprocessing.save_stats"><code class="name flex">
<span>def <span class="ident">save_stats</span></span>(<span>spark, idf, master_path, function_name, reread=False, run_type='local')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>spark</code></strong></dt>
<dd>Spark Session</dd>
<dt><strong><code>idf</code></strong></dt>
<dd>input dataframe</dd>
<dt><strong><code>master_path</code></strong></dt>
<dd>Path to master folder under which all statistics will be saved in a csv file format.</dd>
<dt><strong><code>function_name</code></strong></dt>
<dd>Function Name for which statistics need to be saved. file name will be saved as csv</dd>
<dt><strong><code>reread</code></strong></dt>
<dd>(Default value = False)</dd>
<dt><strong><code>run_type</code></strong></dt>
<dd>(Default value = "local")</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None, dataframe saved <br>
run_type: local or emr based on the mode of execution. Default value is kept as local <br>
reread: option to reread. Default value is kept as False</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_stats(spark, idf, master_path, function_name, reread=False,run_type=&#34;local&#34;):
    &#34;&#34;&#34;

    Args:
      spark: Spark Session
      idf: input dataframe
      master_path: Path to master folder under which all statistics will be saved in a csv file format.
      function_name: Function Name for which statistics need to be saved. file name will be saved as csv
      reread:  (Default value = False)
      run_type:  (Default value = &#34;local&#34;)

    Returns:
      None, dataframe saved &lt;br&gt;\
      run_type: local or emr based on the mode of execution. Default value is kept as local &lt;br&gt;\
      reread: option to reread. Default value is kept as False

    &#34;&#34;&#34;

    if run_type == &#34;local&#34;:

        Path(master_path).mkdir(parents=True, exist_ok=True)
        idf.toPandas().to_csv(ends_with(master_path) + function_name + &#34;.csv&#34;,index=False)

    else:

        bucket_name = master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0]
        path_name = master_path.replace(master_path.split(&#34;//&#34;)[0]+&#34;//&#34;+master_path.split(&#34;//&#34;)[1].split(&#34;/&#34;)[0],&#34;&#34;)[1:]
        s3_resource = boto3.resource(&#34;s3&#34;)
        csv_buffer = BytesIO()
        idf.toPandas().to_csv(csv_buffer,index=False)
        s3_resource.Object(bucket_name, ends_with(path_name) + function_name + &#34;.csv&#34;).put(Body=csv_buffer.getvalue())

    if reread:
        odf = spark.read.csv(ends_with(master_path) + function_name + &#34;.csv&#34;, header=True, inferSchema=True)
        return odf</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="anovos.data_report" href="index.html">anovos.data_report</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="anovos.data_report.report_preprocessing.binRange_to_binIdx" href="#anovos.data_report.report_preprocessing.binRange_to_binIdx">binRange_to_binIdx</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.charts_to_objects" href="#anovos.data_report.report_preprocessing.charts_to_objects">charts_to_objects</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.edit_binRange" href="#anovos.data_report.report_preprocessing.edit_binRange">edit_binRange</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.ends_with" href="#anovos.data_report.report_preprocessing.ends_with">ends_with</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.f_edit_binRange" href="#anovos.data_report.report_preprocessing.f_edit_binRange">f_edit_binRange</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.plot_comparative_drift" href="#anovos.data_report.report_preprocessing.plot_comparative_drift">plot_comparative_drift</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.plot_eventRate" href="#anovos.data_report.report_preprocessing.plot_eventRate">plot_eventRate</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.plot_frequency" href="#anovos.data_report.report_preprocessing.plot_frequency">plot_frequency</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.plot_outlier" href="#anovos.data_report.report_preprocessing.plot_outlier">plot_outlier</a></code></li>
<li><code><a title="anovos.data_report.report_preprocessing.save_stats" href="#anovos.data_report.report_preprocessing.save_stats">save_stats</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>