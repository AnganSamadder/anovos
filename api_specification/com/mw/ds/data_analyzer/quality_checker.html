<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>com.mw.ds.data_analyzer.quality_checker API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>com.mw.ds.data_analyzer.quality_checker</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import re
import warnings

from com.mw.ds.data_analyzer.stats_generator import uniqueCount_computation, missingCount_computation, mode_computation, \
    measures_of_cardinality
from com.mw.ds.data_ingest.data_ingest import read_dataset
from com.mw.ds.data_transformer.transformers import imputation_MMM
from com.mw.ds.shared.spark import *
from com.mw.ds.shared.utils import attributeType_segregation, transpose_dataframe, get_dtype
from pyspark.sql import functions as F
from pyspark.sql import types as T


def duplicate_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: list or string of col names separated by |),\
    all - to include all non-array columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: True if rows to be removed else False (Default value = False)
      print_impact:  (Default value = False)

    Returns:
      Filtered Dataframe, Analysis Dataframe

    &#34;&#34;&#34;
    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    odf_tmp = idf.drop_duplicates(subset=list_of_cols)
    odf = odf_tmp if treatment else idf

    odf_print = spark.createDataFrame([[&#34;rows_count&#34;, idf.count()], [&#34;unique_rows_count&#34;, odf_tmp.count()]],
                                      schema=[&#39;metric&#39;, &#39;value&#39;])

    if print_impact:
        print(&#34;No. of Rows: &#34; + str(idf.count()))
        print(&#34;No. of UNIQUE Rows: &#34; + str(odf_tmp.count()))

    return odf, odf_print


def nullRows_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, treatment_threshold=0.8,
                       print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: list or string of col names separated by |),\
    all - to include all non-array columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: True if rows to be removed else False (Default value = False)
      treatment_threshold: of columns allowed to be Null per row, No row removal if treatment_threshold = 1 (Default value = 0.8)
      print_impact:  (Default value = False)

    Returns:
      Output Dataframe (after row removal),\
      Analysis Dataframe [null_cols_count, row_count,row_pct]

    &#34;&#34;&#34;

    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if (treatment_threshold &lt; 0) | (treatment_threshold &gt; 1):
        raise TypeError(&#39;Invalid input for Treatment Threshold Value&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    def null_count(*cols):
        &#34;&#34;&#34;

        Args:
          *cols: 

        Returns:

        &#34;&#34;&#34;
        return cols.count(None)

    f_null_count = F.udf(null_count, T.LongType())

    odf_tmp = idf.withColumn(&#34;null_cols_count&#34;, f_null_count(*list_of_cols)) \
        .withColumn(&#39;flagged&#39;, F.when(F.col(&#34;null_cols_count&#34;) &gt; (len(list_of_cols) * treatment_threshold), 1) \
                    .otherwise(0))

    if not (treatment) | (treatment_threshold == 1):
        odf = idf
    else:
        odf = odf_tmp.where(F.col(&#34;flagged&#34;) == 0).drop(*[&#34;null_cols_count&#34;, &#34;flagged&#34;])

    odf_print = odf_tmp.groupBy(&#34;null_cols_count&#34;, &#34;flagged&#34;).agg(F.count(F.lit(1)).alias(&#39;row_count&#39;)) \
        .withColumn(&#39;row_pct&#39;, F.round(F.col(&#39;row_count&#39;) / float(idf.count()), 4)) \
        .select(&#39;null_cols_count&#39;, &#39;row_count&#39;, &#39;row_pct&#39;, &#39;flagged&#39;)
    if print_impact:
        odf_print.orderBy(&#39;null_cols_count&#39;).show(odf.count())

    return odf, odf_print


def nullColumns_detection(idf, list_of_cols=&#39;missing&#39;, drop_cols=[], treatment=False, treatment_method=&#39;row_removal&#39;,
                          treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: list of columns (in list format or string separated by |),\
    all - to include all non-array columns (excluding drop_cols),\
    missing - all feautures with missing values (excluding drop_cols) (Default value = &#39;missing&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, Imputation/Dropna/Drop Column based on treatment_method (Default value = False)
      treatment_method: MMM, row_removal or column_removal (more methods to be added soon) (Default value = &#39;row_removal&#39;)
      treatment_configs: All arguments of treatment_method functions in dictionary format (Default value = {})
      stats_missing: read_dataset arguments to read pre-saved statistics on missing count/pct (dictionary format) (Default value = {})
      stats_unique: read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})
      stats_mode: read_dataset arguments to read pre-saved statistics on mode (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Imputed Dataframe

    &#34;&#34;&#34;
    if stats_missing == {}:
        odf_print = missingCount_computation(idf)
    else:
        odf_print = read_dataset(**stats_missing).select(&#39;attribute&#39;, &#39;missing_count&#39;, &#39;missing_pct&#39;)

    missing_cols = odf_print.where(F.col(&#39;missing_count&#39;) &gt; 0).select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()

    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if list_of_cols == &#34;missing&#34;:
        list_of_cols = missing_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if len(list_of_cols) == 0:
        warnings.warn(&#34;No Action Performed - Imputation&#34;)
        return idf
    if any(x not in idf.columns for x in list_of_cols):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if treatment_method not in (&#39;MMM&#39;, &#39;row_removal&#39;, &#39;column_removal&#39;):
        raise TypeError(&#39;Invalid input for method_type&#39;)

    odf_print = odf_print.where(F.col(&#39;attribute&#39;).isin(list_of_cols))

    if treatment:

        if treatment_method == &#39;column_removal&#39;:
            remove_cols = odf_print.where(F.col(&#39;attribute&#39;).isin(list_of_cols)) \
                .where(F.col(&#39;missing_pct&#39;) &gt; treatment_configs[&#39;treatment_threshold&#39;]) \
                .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            odf = idf.drop(*remove_cols)
            if print_impact:
                print(&#34;Removed Columns: &#34;, remove_cols)

        if treatment_method == &#39;row_removal&#39;:
            remove_cols = odf_print.where(F.col(&#39;attribute&#39;).isin(list_of_cols)) \
                .where(F.col(&#39;missing_pct&#39;) == 1.0) \
                .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            list_of_cols = [e for e in list_of_cols if e not in remove_cols]
            odf = idf.dropna(subset=list_of_cols)

            if print_impact:
                odf_print.show(len(list_of_cols))
                print(&#34;Before Count: &#34; + str(idf.count()))
                print(&#34;After Count: &#34; + str(odf.count()))

        if treatment_method == &#39;MMM&#39;:
            if stats_unique == {}:
                remove_cols = uniqueCount_computation(idf_sample, list_of_cols).where(F.col(&#39;unique_values&#39;) &lt; 2) \
                    .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            else:
                remove_cols = read_dataset(**stats_unique).where(F.col(&#39;unique_values&#39;) &lt; 2) \
                    .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            list_of_cols = [e for e in list_of_cols if e not in remove_cols]
            odf = imputation_MMM(idf, list_of_cols, **treatment_configs, stats_missing=stats_missing,
                                 stats_mode=stats_mode, print_impact=print_impact)
    else:
        odf = idf

    return odf, odf_print


def outlier_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], detection_side=&#39;upper&#39;,
                      detection_configs={&#39;pctile_lower&#39;: 0.05, &#39;pctile_upper&#39;: 0.95,
                                         &#39;stdev_lower&#39;: 3.0, &#39;stdev_upper&#39;: 3.0,
                                         &#39;IQR_lower&#39;: 1.5, &#39;IQR_upper&#39;: 1.5,
                                         &#39;min_validation&#39;: 2},
                      treatment=False, treatment_type=&#39;value_replacement&#39;, pre_existing_model=False,
                      model_path=&#34;NA&#34;, output_mode=&#39;replace&#39;, stats_unique={}, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: Numerical Columns (list or string of col names separated by |),\
    all - to include all numerical columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      detection_side: upper, lower, both (Default value = &#39;upper&#39;)
      detection_configs: dictionary format - upper &amp; lower bound for each methodology.\
    If an attribute value is less (more) than its derived lower (upper) bound value, \
    it is considered as outlier by a methodology.\
    A attribute value is outliered if it is declared as oultlier by atleast &#39;min_validation&#39; methodologies.\
    (Default value = {&#39;pctile_lower&#39;: 0.05, &#39;pctile_upper&#39;: 0.95, &#39;stdev_lower&#39;: 3.0, &#39;stdev_upper&#39;: 3.0, IQR_lower&#39;: 1.5, &#39;IQR_upper&#39;: 1.5, &#39;min_validation&#39;: 2})
      treatment: if True, cleaning based on treatment_method (Default value = False)
      treatment_type: null_replacement, row_removal, value_replacement (Default value = &#39;value_replacement&#39;)
      pre_existing_model: outlier value for each attribute. True if model files exists already, False Otherwise (Default value = False)
      model_path: If pre_existing_model is True, this argument is path for presaved model file.\
    If pre_existing_model is False, this field can be used for saving the model file.\
    param NA means there is neither pre_existing_model nor there is a need to save one. (Default value = &#34;NA&#34;)
      output_mode: replace or append (Default value = &#39;replace&#39;)
      stats_unique: read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Output Dataframe (after outlier treatment),\
      Analysis Dataframe [attribute, lower_outliers, upper_outliers]

    &#34;&#34;&#34;

    num_cols = attributeType_segregation(idf)[0]
    if len(num_cols) == 0:
        warnings.warn(&#34;No Outlier Check&#34;)
        odf = idf
        schema = T.StructType([T.StructField(&#39;attribute&#39;, T.StringType(), True),
                               T.StructField(&#39;lower_outliers&#39;, T.StringType(), True),
                               T.StructField(&#39;upper_outliers&#39;, T.StringType(), True)])
        odf_print = spark.sparkContext.emptyRDD().toDF(schema)
        return odf, odf_print
    if list_of_cols == &#39;all&#39;:
        list_of_cols = num_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    if stats_unique == {}:
        remove_cols = uniqueCount_computation(idf, list_of_cols).where(F.col(&#39;unique_values&#39;) &lt; 2) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
    else:
        remove_cols = read_dataset(**stats_unique).where(F.col(&#39;unique_values&#39;) &lt; 2) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()

    list_of_cols = [e for e in list_of_cols if e not in (drop_cols + remove_cols)]

    if any(x not in num_cols for x in list_of_cols):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if detection_side not in (&#39;upper&#39;, &#39;lower&#39;, &#39;both&#39;):
        raise TypeError(&#39;Invalid input for detection_side&#39;)
    if treatment_type not in (&#39;null_replacement&#39;, &#39;row_removal&#39;, &#39;value_replacement&#39;):
        raise TypeError(&#39;Invalid input for treatment_type&#39;)
    if output_mode not in (&#39;replace&#39;, &#39;append&#39;):
        raise TypeError(&#39;Invalid input for output_mode&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)
    if str(pre_existing_model).lower() == &#39;true&#39;:
        pre_existing_model = True
    elif str(pre_existing_model).lower() == &#39;false&#39;:
        pre_existing_model = False
    else:
        raise TypeError(&#39;Non-Boolean input for pre_existing_model&#39;)
    for arg in [&#39;pctile_lower&#39;, &#39;pctile_upper&#39;]:
        if arg in detection_configs:
            if (detection_configs[arg] &lt; 0) | (detection_configs[arg] &gt; 1):
                raise TypeError(&#39;Invalid input for &#39; + arg)

    recast_cols = []
    recast_type = []
    for i in list_of_cols:
        if get_dtype(idf, i).startswith(&#39;decimal&#39;):
            idf = idf.withColumn(i, F.col(i).cast(T.DoubleType()))
            recast_cols.append(i)
            recast_type.append(get_dtype(idf, i))

    if pre_existing_model:
        df_model = sqlContext.read.parquet(model_path + &#34;/outlier_numcols&#34;)
        params = []
        for i in list_of_cols:
            mapped_value = df_model.where(F.col(&#39;attribute&#39;) == i).select(&#39;parameters&#39;) \
                .rdd.flatMap(lambda x: x).collect()[0]
            params.append(mapped_value)
    else:
        detection_configs[&#39;pctile_lower&#39;] = detection_configs[&#39;pctile_lower&#39;] or 0.0
        detection_configs[&#39;pctile_upper&#39;] = detection_configs[&#39;pctile_upper&#39;] or 1.0
        pctile_params = idf.approxQuantile(list_of_cols, [detection_configs[&#39;pctile_lower&#39;],
                                                          detection_configs[&#39;pctile_upper&#39;]], 0.01)
        detection_configs[&#39;stdev_lower&#39;] = detection_configs[&#39;stdev_lower&#39;] or detection_configs[&#39;stdev_upper&#39;]
        detection_configs[&#39;stdev_upper&#39;] = detection_configs[&#39;stdev_upper&#39;] or detection_configs[&#39;stdev_lower&#39;]
        stdev_params = []
        for i in list_of_cols:
            mean, stdev = idf.select(F.mean(i), F.stddev(i)).first()
            stdev_params.append(
                [mean - detection_configs[&#39;stdev_lower&#39;] * stdev, mean + detection_configs[&#39;stdev_upper&#39;] * stdev])

        detection_configs[&#39;IQR_lower&#39;] = detection_configs[&#39;IQR_lower&#39;] or detection_configs[&#39;IQR_upper&#39;]
        detection_configs[&#39;IQR_upper&#39;] = detection_configs[&#39;IQR_upper&#39;] or detection_configs[&#39;IQR_lower&#39;]
        quantiles = idf.approxQuantile(list_of_cols, [0.25, 0.75], 0.01)
        IQR_params = [[e[0] - detection_configs[&#39;IQR_lower&#39;] * (e[1] - e[0]),
                       e[1] + detection_configs[&#39;IQR_upper&#39;] * (e[1] - e[0])] for e in quantiles]
        n = detection_configs[&#39;min_validation&#39;]
        params = [[sorted([x[0], y[0], z[0]], reverse=True)[n - 1], sorted([x[1], y[1], z[1]])[n - 1]] for x, y, z in
                  list(zip(pctile_params, stdev_params, IQR_params))]

        # Saving model File if required
        if model_path != &#34;NA&#34;:
            df_model = spark.createDataFrame(zip(list_of_cols, params), schema=[&#39;attribute&#39;, &#39;parameters&#39;])
            df_model.coalesce(1).write.parquet(model_path + &#34;/outlier_numcols&#34;, mode=&#39;overwrite&#39;)

    for i, j in zip(recast_cols, recast_type):
        idf = idf.withColumn(i, F.col(i).cast(j))

    def composite_outlier(*v):
        &#34;&#34;&#34;

        Args:
          *v: 

        Returns:

        &#34;&#34;&#34;
        output = []
        for idx, e in enumerate(v):
            if e is None:
                output.append(None)
                continue
            if detection_side in (&#39;upper&#39;, &#39;both&#39;):
                if e &gt; params[idx][1]:
                    output.append(1)
                    continue
            if detection_side in (&#39;lower&#39;, &#39;both&#39;):
                if e &lt; params[idx][0]:
                    output.append(-1)
                    continue
            output.append(0)
        return output

    f_composite_outlier = F.udf(composite_outlier, T.ArrayType(T.IntegerType()))

    odf = idf.withColumn(&#34;outliered&#34;, f_composite_outlier(*list_of_cols))
    odf.persist()
    output_print = []
    for index, i in enumerate(list_of_cols):
        odf = odf.withColumn(i + &#34;_outliered&#34;, F.col(&#39;outliered&#39;)[index])
        output_print.append(
            [i, odf.where(F.col(i + &#34;_outliered&#34;) == -1).count(), odf.where(F.col(i + &#34;_outliered&#34;) == 1).count()])

        if treatment &amp; (treatment_type in (&#39;value_replacement&#39;, &#39;null_replacement&#39;)):
            replace_vals = {&#39;value_replacement&#39;: [params[index][0], params[index][1]], &#39;null_replacement&#39;: [None, None]}
            odf = odf.withColumn(i + &#34;_outliered&#34;, F.when(F.col(i + &#34;_outliered&#34;) == 1, replace_vals[treatment_type][1]) \
                                 .otherwise(F.when(F.col(i + &#34;_outliered&#34;) == -1, replace_vals[treatment_type][0]) \
                                            .otherwise(F.col(i))))
            if output_mode == &#39;replace&#39;:
                odf = odf.drop(i).withColumnRenamed(i + &#34;_outliered&#34;, i)

    odf = odf.drop(&#34;outliered&#34;)

    if treatment &amp; (treatment_type == &#39;row_removal&#39;):
        for index, i in enumerate(list_of_cols):
            odf = odf.where((F.col(i + &#34;_outliered&#34;) == 0) | (F.col(i + &#34;_outliered&#34;).isNull())).drop(i + &#34;_outliered&#34;)

    if not (treatment):
        odf = idf

    odf_print = spark.createDataFrame(output_print, schema=[&#39;attribute&#39;, &#39;lower_outliers&#39;, &#39;upper_outliers&#39;])
    if print_impact:
        odf_print.show(len(list_of_cols))

    return odf, odf_print


def IDness_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, treatment_threshold=1.0, stats_unique={},
                     print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: Categorical Columns (list or string of col names separated by |),\
    all - to include all categorical columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, delete columns based on treatment_threshold (Default value = False)
      treatment_threshold: 0-1&gt; Remove categorical column if no. of unique values is more than X% of total rows. (Default value = 1.0)
      stats_unique: read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Filtered Dataframe (if treated), Analysis Dataframe [attribute, unique_values, IDness]

    &#34;&#34;&#34;

    cat_cols = attributeType_segregation(idf)[1]
    if list_of_cols == &#39;all&#39;:
        list_of_cols = cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in cat_cols for x in list_of_cols):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if len(list_of_cols) == 0:
        warnings.warn(&#34;No IDness Check&#34;)
        odf = idf
        schema = T.StructType([T.StructField(&#39;attribute&#39;, T.StringType(), True),
                               T.StructField(&#39;unique_values&#39;, T.StringType(), True),
                               T.StructField(&#39;IDness&#39;, T.StringType(), True),
                               T.StructField(&#39;flagged&#39;, T.StringType(), True)])
        odf_print = spark.sparkContext.emptyRDD().toDF(schema)
        return odf, odf_print
    if (treatment_threshold &lt; 0) | (treatment_threshold &gt; 1):
        raise TypeError(&#39;Invalid input for Treatment Threshold Value&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    if stats_unique == {}:
        odf_print = measures_of_cardinality(idf, list_of_cols) \
            .withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))
    else:
        odf_print = read_dataset(**stats_unique).withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))

    if treatment:
        remove_cols = odf_print.where(F.col(&#39;IDness&#39;) &gt;= treatment_threshold) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
        odf = idf.drop(*remove_cols)
        odf_print = odf_print.withColumn(&#39;flagged&#39;, F.when(F.col(&#39;IDness&#39;) &gt;= treatment_threshold, 1).otherwise(0))
    else:
        odf = idf

    if print_impact:
        odf_print.show(len(list_of_cols))
        if treatment:
            print(&#34;Removed Columns: &#34;, remove_cols)

    return odf, odf_print


def biasedness_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, treatment_threshold=1.0, stats_mode={},
                         print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: List of columns (in list format or string separated by |),\
    all - to include all non-array columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, delete columns based on treatment_threshold (Default value = False)
      treatment_threshold: 0-1&gt; Remove categorical column if most freq value is in more than X% of total rows. (Default value = 1.0)
      stats_mode: read_dataset arguments to read pre-saved statistics on mode (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Filtered Dataframe (if treated), Analysis Dataframe [attribute, mode, mode_pct]

    &#34;&#34;&#34;

    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if (treatment_threshold &lt; 0) | (treatment_threshold &gt; 1):
        raise TypeError(&#39;Invalid input for Treatment Threshold Value&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    if stats_mode == {}:
        odf_print = transpose_dataframe(idf.select(list_of_cols).summary(&#34;count&#34;), &#39;summary&#39;) \
            .withColumnRenamed(&#39;key&#39;, &#39;attribute&#39;) \
            .join(mode_computation(idf, list_of_cols), &#39;attribute&#39;, &#39;full_outer&#39;) \
            .withColumn(&#39;mode_pct&#39;, F.round(F.col(&#39;mode_rows&#39;) / F.col(&#39;count&#39;).cast(T.DoubleType()), 4)) \
            .select(&#39;attribute&#39;, &#39;mode&#39;, &#39;mode_pct&#39;).withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))
    else:
        odf_print = read_dataset(**stats_mode).select(&#39;attribute&#39;, &#39;mode&#39;, &#39;mode_pct&#39;).withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))

    if treatment:
        remove_cols = odf_print.where((F.col(&#39;mode_pct&#39;) &gt;= treatment_threshold) | (F.col(&#39;mode_pct&#39;).isNull())) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
        odf = idf.drop(*remove_cols)
        odf_print = odf_print.withColumn(&#39;flagged&#39;,
                                         F.when(
                                             (F.col(&#39;mode_pct&#39;) &gt;= treatment_threshold) | (F.col(&#39;mode_pct&#39;).isNull()),
                                             1).otherwise(0))
    else:
        odf = idf

    if print_impact:
        odf_print.show(len(list_of_cols))
        if treatment:
            print(&#34;Removed Columns: &#34;, remove_cols)

    return odf, odf_print


def invalidEntries_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False,
                             output_mode=&#39;replace&#39;, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: List of Distrete (Categorical + Integer) columns (list or string of col names separated by |),\
    all - to include all valid columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, replace invalid values by Null (false positives possible) (Default value = False)
      output_mode: replace or append (Default value = &#39;replace&#39;)
      print_impact:  (Default value = False)

    Returns:
      Output Dataframe (if treated) else Input Dataframe,\
      Analysis Dataframe [attribute, invalid_entries, invalid_count, invalid_pct]

    &#34;&#34;&#34;
    if list_of_cols == &#39;all&#39;:
        list_of_cols = []
        for i in idf.dtypes:
            if (i[1] in (&#39;string&#39;, &#39;int&#39;, &#39;bigint&#39;, &#39;long&#39;)):
                list_of_cols.append(i[0])
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if output_mode not in (&#39;replace&#39;, &#39;append&#39;):
        raise TypeError(&#39;Invalid input for output_mode&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    null_vocab = [&#39;&#39;, &#39; &#39;, &#39;nan&#39;, &#39;null&#39;, &#39;na&#39;, &#39;inf&#39;, &#39;n/a&#39;, &#39;not defined&#39;, &#39;none&#39;, &#39;undefined&#39;, &#39;blank&#39;]
    specialChars_vocab = [&#34;&amp;&#34;, &#34;$&#34;, &#34;;&#34;, &#34;:&#34;, &#34;.&#34;, &#34;,&#34;, &#34;*&#34;, &#34;#&#34;, &#34;@&#34;, &#34;_&#34;, &#34;?&#34;, &#34;%&#34;, &#34;!&#34;, &#34;^&#34;, &#34;(&#34;, &#34;)&#34;, &#34;-&#34;, &#34;/&#34;, &#34;&#39;&#34;]

    def detect(*v):
        &#34;&#34;&#34;

        Args:
          *v: 

        Returns:

        &#34;&#34;&#34;
        output = []
        for idx, e in enumerate(v):
            if e is None:
                output.append(None)
                continue
            e = str(e).lower().strip()
            # Null &amp; Special Chars Search
            if e in (null_vocab + specialChars_vocab):
                output.append(1)
                continue
            # Consecutive Identical Chars Search
            regex = &#34;\\b([a-zA-Z0-9])\\1\\1+\\b&#34;
            p = re.compile(regex)
            if (re.search(p, e)):
                output.append(1)
                continue
            # Ordered Chars Search
            l = len(e)
            check = 0
            if l &gt;= 3:
                for i in range(1, l):
                    if ord(e[i]) - ord(e[i - 1]) != 1:
                        output.append(0)
                        check = 1
                        break
                if check == 1:
                    continue
                else:
                    output.append(1)
                    continue
            else:
                output.append(0)
                continue
        return output

    f_detect = F.udf(detect, T.ArrayType(T.LongType()))

    odf = idf.withColumn(&#34;invalid&#34;, f_detect(*list_of_cols))
    odf.persist()
    output_print = []
    for index, i in enumerate(list_of_cols):
        tmp = odf.withColumn(i + &#34;_invalid&#34;, F.col(&#39;invalid&#39;)[index])
        invalid = tmp.where(F.col(i + &#34;_invalid&#34;) == 1).select(i).distinct().rdd.flatMap(lambda x: x).collect()
        invalid = [str(x) for x in invalid]
        invalid_count = tmp.where(F.col(i + &#34;_invalid&#34;) == 1).count()
        output_print.append([i, &#39;|&#39;.join(invalid), invalid_count, round(invalid_count / idf.count(), 4)])

    if treatment:
        for index, i in enumerate(list_of_cols):
            odf = odf.withColumn(i + &#34;_invalid&#34;, F.when(F.col(&#39;invalid&#39;)[index] == 1, None).otherwise(F.col(i)))
            if output_mode == &#39;replace&#39;:
                odf = odf.drop(i).withColumnRenamed(i + &#34;_invalid&#34;, i)
        odf = odf.drop(&#34;invalid&#34;)
    else:
        odf = idf

    odf_print = spark.createDataFrame(output_print,
                                      schema=[&#39;attribute&#39;, &#39;invalid_entries&#39;, &#39;invalid_count&#39;, &#39;invalid_pct&#39;])
    if print_impact:
        odf_print.show(len(list_of_cols))

    return odf, odf_print</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="com.mw.ds.data_analyzer.quality_checker.IDness_detection"><code class="name flex">
<span>def <span class="ident">IDness_detection</span></span>(<span>idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=1.0, stats_unique={}, print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>Categorical Columns (list or string of col names separated by |),
all - to include all categorical columns (excluding drop_cols) (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>If True, delete columns based on treatment_threshold (Default value = False)</dd>
<dt><strong><code>treatment_threshold</code></strong></dt>
<dd>0-1&gt; Remove categorical column if no. of unique values is more than X% of total rows. (Default value = 1.0)</dd>
<dt><strong><code>stats_unique</code></strong></dt>
<dd>read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Filtered Dataframe (if treated), Analysis Dataframe [attribute, unique_values, IDness]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def IDness_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, treatment_threshold=1.0, stats_unique={},
                     print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: Categorical Columns (list or string of col names separated by |),\
    all - to include all categorical columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, delete columns based on treatment_threshold (Default value = False)
      treatment_threshold: 0-1&gt; Remove categorical column if no. of unique values is more than X% of total rows. (Default value = 1.0)
      stats_unique: read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Filtered Dataframe (if treated), Analysis Dataframe [attribute, unique_values, IDness]

    &#34;&#34;&#34;

    cat_cols = attributeType_segregation(idf)[1]
    if list_of_cols == &#39;all&#39;:
        list_of_cols = cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in cat_cols for x in list_of_cols):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if len(list_of_cols) == 0:
        warnings.warn(&#34;No IDness Check&#34;)
        odf = idf
        schema = T.StructType([T.StructField(&#39;attribute&#39;, T.StringType(), True),
                               T.StructField(&#39;unique_values&#39;, T.StringType(), True),
                               T.StructField(&#39;IDness&#39;, T.StringType(), True),
                               T.StructField(&#39;flagged&#39;, T.StringType(), True)])
        odf_print = spark.sparkContext.emptyRDD().toDF(schema)
        return odf, odf_print
    if (treatment_threshold &lt; 0) | (treatment_threshold &gt; 1):
        raise TypeError(&#39;Invalid input for Treatment Threshold Value&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    if stats_unique == {}:
        odf_print = measures_of_cardinality(idf, list_of_cols) \
            .withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))
    else:
        odf_print = read_dataset(**stats_unique).withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))

    if treatment:
        remove_cols = odf_print.where(F.col(&#39;IDness&#39;) &gt;= treatment_threshold) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
        odf = idf.drop(*remove_cols)
        odf_print = odf_print.withColumn(&#39;flagged&#39;, F.when(F.col(&#39;IDness&#39;) &gt;= treatment_threshold, 1).otherwise(0))
    else:
        odf = idf

    if print_impact:
        odf_print.show(len(list_of_cols))
        if treatment:
            print(&#34;Removed Columns: &#34;, remove_cols)

    return odf, odf_print</code></pre>
</details>
</dd>
<dt id="com.mw.ds.data_analyzer.quality_checker.biasedness_detection"><code class="name flex">
<span>def <span class="ident">biasedness_detection</span></span>(<span>idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=1.0, stats_mode={}, print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>List of columns (in list format or string separated by |),
all - to include all non-array columns (excluding drop_cols) (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>If True, delete columns based on treatment_threshold (Default value = False)</dd>
<dt><strong><code>treatment_threshold</code></strong></dt>
<dd>0-1&gt; Remove categorical column if most freq value is in more than X% of total rows. (Default value = 1.0)</dd>
<dt><strong><code>stats_mode</code></strong></dt>
<dd>read_dataset arguments to read pre-saved statistics on mode (dictionary format) (Default value = {})</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Filtered Dataframe (if treated), Analysis Dataframe [attribute, mode, mode_pct]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def biasedness_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, treatment_threshold=1.0, stats_mode={},
                         print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: List of columns (in list format or string separated by |),\
    all - to include all non-array columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, delete columns based on treatment_threshold (Default value = False)
      treatment_threshold: 0-1&gt; Remove categorical column if most freq value is in more than X% of total rows. (Default value = 1.0)
      stats_mode: read_dataset arguments to read pre-saved statistics on mode (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Filtered Dataframe (if treated), Analysis Dataframe [attribute, mode, mode_pct]

    &#34;&#34;&#34;

    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if (treatment_threshold &lt; 0) | (treatment_threshold &gt; 1):
        raise TypeError(&#39;Invalid input for Treatment Threshold Value&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    if stats_mode == {}:
        odf_print = transpose_dataframe(idf.select(list_of_cols).summary(&#34;count&#34;), &#39;summary&#39;) \
            .withColumnRenamed(&#39;key&#39;, &#39;attribute&#39;) \
            .join(mode_computation(idf, list_of_cols), &#39;attribute&#39;, &#39;full_outer&#39;) \
            .withColumn(&#39;mode_pct&#39;, F.round(F.col(&#39;mode_rows&#39;) / F.col(&#39;count&#39;).cast(T.DoubleType()), 4)) \
            .select(&#39;attribute&#39;, &#39;mode&#39;, &#39;mode_pct&#39;).withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))
    else:
        odf_print = read_dataset(**stats_mode).select(&#39;attribute&#39;, &#39;mode&#39;, &#39;mode_pct&#39;).withColumn(&#39;flagged&#39;, F.lit(&#34;-&#34;))

    if treatment:
        remove_cols = odf_print.where((F.col(&#39;mode_pct&#39;) &gt;= treatment_threshold) | (F.col(&#39;mode_pct&#39;).isNull())) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
        odf = idf.drop(*remove_cols)
        odf_print = odf_print.withColumn(&#39;flagged&#39;,
                                         F.when(
                                             (F.col(&#39;mode_pct&#39;) &gt;= treatment_threshold) | (F.col(&#39;mode_pct&#39;).isNull()),
                                             1).otherwise(0))
    else:
        odf = idf

    if print_impact:
        odf_print.show(len(list_of_cols))
        if treatment:
            print(&#34;Removed Columns: &#34;, remove_cols)

    return odf, odf_print</code></pre>
</details>
</dd>
<dt id="com.mw.ds.data_analyzer.quality_checker.duplicate_detection"><code class="name flex">
<span>def <span class="ident">duplicate_detection</span></span>(<span>idf, list_of_cols='all', drop_cols=[], treatment=False, print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>list or string of col names separated by |),
all - to include all non-array columns (excluding drop_cols) (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>True if rows to be removed else False (Default value = False)</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Filtered Dataframe, Analysis Dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def duplicate_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: list or string of col names separated by |),\
    all - to include all non-array columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: True if rows to be removed else False (Default value = False)
      print_impact:  (Default value = False)

    Returns:
      Filtered Dataframe, Analysis Dataframe

    &#34;&#34;&#34;
    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    odf_tmp = idf.drop_duplicates(subset=list_of_cols)
    odf = odf_tmp if treatment else idf

    odf_print = spark.createDataFrame([[&#34;rows_count&#34;, idf.count()], [&#34;unique_rows_count&#34;, odf_tmp.count()]],
                                      schema=[&#39;metric&#39;, &#39;value&#39;])

    if print_impact:
        print(&#34;No. of Rows: &#34; + str(idf.count()))
        print(&#34;No. of UNIQUE Rows: &#34; + str(odf_tmp.count()))

    return odf, odf_print</code></pre>
</details>
</dd>
<dt id="com.mw.ds.data_analyzer.quality_checker.invalidEntries_detection"><code class="name flex">
<span>def <span class="ident">invalidEntries_detection</span></span>(<span>idf, list_of_cols='all', drop_cols=[], treatment=False, output_mode='replace', print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>List of Distrete (Categorical + Integer) columns (list or string of col names separated by |),
all - to include all valid columns (excluding drop_cols) (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>If True, replace invalid values by Null (false positives possible) (Default value = False)</dd>
<dt><strong><code>output_mode</code></strong></dt>
<dd>replace or append (Default value = 'replace')</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output Dataframe (if treated) else Input Dataframe,
Analysis Dataframe [attribute, invalid_entries, invalid_count, invalid_pct]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def invalidEntries_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False,
                             output_mode=&#39;replace&#39;, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: List of Distrete (Categorical + Integer) columns (list or string of col names separated by |),\
    all - to include all valid columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, replace invalid values by Null (false positives possible) (Default value = False)
      output_mode: replace or append (Default value = &#39;replace&#39;)
      print_impact:  (Default value = False)

    Returns:
      Output Dataframe (if treated) else Input Dataframe,\
      Analysis Dataframe [attribute, invalid_entries, invalid_count, invalid_pct]

    &#34;&#34;&#34;
    if list_of_cols == &#39;all&#39;:
        list_of_cols = []
        for i in idf.dtypes:
            if (i[1] in (&#39;string&#39;, &#39;int&#39;, &#39;bigint&#39;, &#39;long&#39;)):
                list_of_cols.append(i[0])
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if output_mode not in (&#39;replace&#39;, &#39;append&#39;):
        raise TypeError(&#39;Invalid input for output_mode&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    null_vocab = [&#39;&#39;, &#39; &#39;, &#39;nan&#39;, &#39;null&#39;, &#39;na&#39;, &#39;inf&#39;, &#39;n/a&#39;, &#39;not defined&#39;, &#39;none&#39;, &#39;undefined&#39;, &#39;blank&#39;]
    specialChars_vocab = [&#34;&amp;&#34;, &#34;$&#34;, &#34;;&#34;, &#34;:&#34;, &#34;.&#34;, &#34;,&#34;, &#34;*&#34;, &#34;#&#34;, &#34;@&#34;, &#34;_&#34;, &#34;?&#34;, &#34;%&#34;, &#34;!&#34;, &#34;^&#34;, &#34;(&#34;, &#34;)&#34;, &#34;-&#34;, &#34;/&#34;, &#34;&#39;&#34;]

    def detect(*v):
        &#34;&#34;&#34;

        Args:
          *v: 

        Returns:

        &#34;&#34;&#34;
        output = []
        for idx, e in enumerate(v):
            if e is None:
                output.append(None)
                continue
            e = str(e).lower().strip()
            # Null &amp; Special Chars Search
            if e in (null_vocab + specialChars_vocab):
                output.append(1)
                continue
            # Consecutive Identical Chars Search
            regex = &#34;\\b([a-zA-Z0-9])\\1\\1+\\b&#34;
            p = re.compile(regex)
            if (re.search(p, e)):
                output.append(1)
                continue
            # Ordered Chars Search
            l = len(e)
            check = 0
            if l &gt;= 3:
                for i in range(1, l):
                    if ord(e[i]) - ord(e[i - 1]) != 1:
                        output.append(0)
                        check = 1
                        break
                if check == 1:
                    continue
                else:
                    output.append(1)
                    continue
            else:
                output.append(0)
                continue
        return output

    f_detect = F.udf(detect, T.ArrayType(T.LongType()))

    odf = idf.withColumn(&#34;invalid&#34;, f_detect(*list_of_cols))
    odf.persist()
    output_print = []
    for index, i in enumerate(list_of_cols):
        tmp = odf.withColumn(i + &#34;_invalid&#34;, F.col(&#39;invalid&#39;)[index])
        invalid = tmp.where(F.col(i + &#34;_invalid&#34;) == 1).select(i).distinct().rdd.flatMap(lambda x: x).collect()
        invalid = [str(x) for x in invalid]
        invalid_count = tmp.where(F.col(i + &#34;_invalid&#34;) == 1).count()
        output_print.append([i, &#39;|&#39;.join(invalid), invalid_count, round(invalid_count / idf.count(), 4)])

    if treatment:
        for index, i in enumerate(list_of_cols):
            odf = odf.withColumn(i + &#34;_invalid&#34;, F.when(F.col(&#39;invalid&#39;)[index] == 1, None).otherwise(F.col(i)))
            if output_mode == &#39;replace&#39;:
                odf = odf.drop(i).withColumnRenamed(i + &#34;_invalid&#34;, i)
        odf = odf.drop(&#34;invalid&#34;)
    else:
        odf = idf

    odf_print = spark.createDataFrame(output_print,
                                      schema=[&#39;attribute&#39;, &#39;invalid_entries&#39;, &#39;invalid_count&#39;, &#39;invalid_pct&#39;])
    if print_impact:
        odf_print.show(len(list_of_cols))

    return odf, odf_print</code></pre>
</details>
</dd>
<dt id="com.mw.ds.data_analyzer.quality_checker.nullColumns_detection"><code class="name flex">
<span>def <span class="ident">nullColumns_detection</span></span>(<span>idf, list_of_cols='missing', drop_cols=[], treatment=False, treatment_method='row_removal', treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>list of columns (in list format or string separated by |),
all - to include all non-array columns (excluding drop_cols),
missing - all feautures with missing values (excluding drop_cols) (Default value = 'missing')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>If True, Imputation/Dropna/Drop Column based on treatment_method (Default value = False)</dd>
<dt><strong><code>treatment_method</code></strong></dt>
<dd>MMM, row_removal or column_removal (more methods to be added soon) (Default value = 'row_removal')</dd>
<dt><strong><code>treatment_configs</code></strong></dt>
<dd>All arguments of treatment_method functions in dictionary format (Default value = {})</dd>
<dt><strong><code>stats_missing</code></strong></dt>
<dd>read_dataset arguments to read pre-saved statistics on missing count/pct (dictionary format) (Default value = {})</dd>
<dt><strong><code>stats_unique</code></strong></dt>
<dd>read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})</dd>
<dt><strong><code>stats_mode</code></strong></dt>
<dd>read_dataset arguments to read pre-saved statistics on mode (dictionary format) (Default value = {})</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Imputed Dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nullColumns_detection(idf, list_of_cols=&#39;missing&#39;, drop_cols=[], treatment=False, treatment_method=&#39;row_removal&#39;,
                          treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: list of columns (in list format or string separated by |),\
    all - to include all non-array columns (excluding drop_cols),\
    missing - all feautures with missing values (excluding drop_cols) (Default value = &#39;missing&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: If True, Imputation/Dropna/Drop Column based on treatment_method (Default value = False)
      treatment_method: MMM, row_removal or column_removal (more methods to be added soon) (Default value = &#39;row_removal&#39;)
      treatment_configs: All arguments of treatment_method functions in dictionary format (Default value = {})
      stats_missing: read_dataset arguments to read pre-saved statistics on missing count/pct (dictionary format) (Default value = {})
      stats_unique: read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})
      stats_mode: read_dataset arguments to read pre-saved statistics on mode (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Imputed Dataframe

    &#34;&#34;&#34;
    if stats_missing == {}:
        odf_print = missingCount_computation(idf)
    else:
        odf_print = read_dataset(**stats_missing).select(&#39;attribute&#39;, &#39;missing_count&#39;, &#39;missing_pct&#39;)

    missing_cols = odf_print.where(F.col(&#39;missing_count&#39;) &gt; 0).select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()

    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if list_of_cols == &#34;missing&#34;:
        list_of_cols = missing_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if len(list_of_cols) == 0:
        warnings.warn(&#34;No Action Performed - Imputation&#34;)
        return idf
    if any(x not in idf.columns for x in list_of_cols):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if treatment_method not in (&#39;MMM&#39;, &#39;row_removal&#39;, &#39;column_removal&#39;):
        raise TypeError(&#39;Invalid input for method_type&#39;)

    odf_print = odf_print.where(F.col(&#39;attribute&#39;).isin(list_of_cols))

    if treatment:

        if treatment_method == &#39;column_removal&#39;:
            remove_cols = odf_print.where(F.col(&#39;attribute&#39;).isin(list_of_cols)) \
                .where(F.col(&#39;missing_pct&#39;) &gt; treatment_configs[&#39;treatment_threshold&#39;]) \
                .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            odf = idf.drop(*remove_cols)
            if print_impact:
                print(&#34;Removed Columns: &#34;, remove_cols)

        if treatment_method == &#39;row_removal&#39;:
            remove_cols = odf_print.where(F.col(&#39;attribute&#39;).isin(list_of_cols)) \
                .where(F.col(&#39;missing_pct&#39;) == 1.0) \
                .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            list_of_cols = [e for e in list_of_cols if e not in remove_cols]
            odf = idf.dropna(subset=list_of_cols)

            if print_impact:
                odf_print.show(len(list_of_cols))
                print(&#34;Before Count: &#34; + str(idf.count()))
                print(&#34;After Count: &#34; + str(odf.count()))

        if treatment_method == &#39;MMM&#39;:
            if stats_unique == {}:
                remove_cols = uniqueCount_computation(idf_sample, list_of_cols).where(F.col(&#39;unique_values&#39;) &lt; 2) \
                    .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            else:
                remove_cols = read_dataset(**stats_unique).where(F.col(&#39;unique_values&#39;) &lt; 2) \
                    .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
            list_of_cols = [e for e in list_of_cols if e not in remove_cols]
            odf = imputation_MMM(idf, list_of_cols, **treatment_configs, stats_missing=stats_missing,
                                 stats_mode=stats_mode, print_impact=print_impact)
    else:
        odf = idf

    return odf, odf_print</code></pre>
</details>
</dd>
<dt id="com.mw.ds.data_analyzer.quality_checker.nullRows_detection"><code class="name flex">
<span>def <span class="ident">nullRows_detection</span></span>(<span>idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>list or string of col names separated by |),
all - to include all non-array columns (excluding drop_cols) (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>True if rows to be removed else False (Default value = False)</dd>
<dt><strong><code>treatment_threshold</code></strong></dt>
<dd>of columns allowed to be Null per row, No row removal if treatment_threshold = 1 (Default value = 0.8)</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output Dataframe (after row removal),
Analysis Dataframe [null_cols_count, row_count,row_pct]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nullRows_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], treatment=False, treatment_threshold=0.8,
                       print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: list or string of col names separated by |),\
    all - to include all non-array columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      treatment: True if rows to be removed else False (Default value = False)
      treatment_threshold: of columns allowed to be Null per row, No row removal if treatment_threshold = 1 (Default value = 0.8)
      print_impact:  (Default value = False)

    Returns:
      Output Dataframe (after row removal),\
      Analysis Dataframe [null_cols_count, row_count,row_pct]

    &#34;&#34;&#34;

    if list_of_cols == &#39;all&#39;:
        num_cols, cat_cols, other_cols = attributeType_segregation(idf)
        list_of_cols = num_cols + cat_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    list_of_cols = [e for e in list_of_cols if e not in drop_cols]

    if any(x not in idf.columns for x in list_of_cols) | (len(list_of_cols) == 0):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if (treatment_threshold &lt; 0) | (treatment_threshold &gt; 1):
        raise TypeError(&#39;Invalid input for Treatment Threshold Value&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)

    def null_count(*cols):
        &#34;&#34;&#34;

        Args:
          *cols: 

        Returns:

        &#34;&#34;&#34;
        return cols.count(None)

    f_null_count = F.udf(null_count, T.LongType())

    odf_tmp = idf.withColumn(&#34;null_cols_count&#34;, f_null_count(*list_of_cols)) \
        .withColumn(&#39;flagged&#39;, F.when(F.col(&#34;null_cols_count&#34;) &gt; (len(list_of_cols) * treatment_threshold), 1) \
                    .otherwise(0))

    if not (treatment) | (treatment_threshold == 1):
        odf = idf
    else:
        odf = odf_tmp.where(F.col(&#34;flagged&#34;) == 0).drop(*[&#34;null_cols_count&#34;, &#34;flagged&#34;])

    odf_print = odf_tmp.groupBy(&#34;null_cols_count&#34;, &#34;flagged&#34;).agg(F.count(F.lit(1)).alias(&#39;row_count&#39;)) \
        .withColumn(&#39;row_pct&#39;, F.round(F.col(&#39;row_count&#39;) / float(idf.count()), 4)) \
        .select(&#39;null_cols_count&#39;, &#39;row_count&#39;, &#39;row_pct&#39;, &#39;flagged&#39;)
    if print_impact:
        odf_print.orderBy(&#39;null_cols_count&#39;).show(odf.count())

    return odf, odf_print</code></pre>
</details>
</dd>
<dt id="com.mw.ds.data_analyzer.quality_checker.outlier_detection"><code class="name flex">
<span>def <span class="ident">outlier_detection</span></span>(<span>idf, list_of_cols='all', drop_cols=[], detection_side='upper', detection_configs={'pctile_lower': 0.05, 'pctile_upper': 0.95, 'stdev_lower': 3.0, 'stdev_upper': 3.0, 'IQR_lower': 1.5, 'IQR_upper': 1.5, 'min_validation': 2}, treatment=False, treatment_type='value_replacement', pre_existing_model=False, model_path='NA', output_mode='replace', stats_unique={}, print_impact=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>idf</code></strong></dt>
<dd>Input Dataframe</dd>
<dt><strong><code>list_of_cols</code></strong></dt>
<dd>Numerical Columns (list or string of col names separated by |),
all - to include all numerical columns (excluding drop_cols) (Default value = 'all')</dd>
<dt><strong><code>drop_cols</code></strong></dt>
<dd>List of columns to be dropped (list or string of col names separated by |) (Default value = [])</dd>
<dt><strong><code>detection_side</code></strong></dt>
<dd>upper, lower, both (Default value = 'upper')</dd>
<dt><strong><code>detection_configs</code></strong></dt>
<dd>dictionary format - upper &amp; lower bound for each methodology.
If an attribute value is less (more) than its derived lower (upper) bound value,
it is considered as outlier by a methodology.
A attribute value is outliered if it is declared as oultlier by atleast 'min_validation' methodologies.
(Default value = {'pctile_lower': 0.05, 'pctile_upper': 0.95, 'stdev_lower': 3.0, 'stdev_upper': 3.0, IQR_lower': 1.5, 'IQR_upper': 1.5, 'min_validation': 2})</dd>
<dt><strong><code>treatment</code></strong></dt>
<dd>if True, cleaning based on treatment_method (Default value = False)</dd>
<dt><strong><code>treatment_type</code></strong></dt>
<dd>null_replacement, row_removal, value_replacement (Default value = 'value_replacement')</dd>
<dt><strong><code>pre_existing_model</code></strong></dt>
<dd>outlier value for each attribute. True if model files exists already, False Otherwise (Default value = False)</dd>
<dt><strong><code>model_path</code></strong></dt>
<dd>If pre_existing_model is True, this argument is path for presaved model file.
If pre_existing_model is False, this field can be used for saving the model file.
param NA means there is neither pre_existing_model nor there is a need to save one. (Default value = "NA")</dd>
<dt><strong><code>output_mode</code></strong></dt>
<dd>replace or append (Default value = 'replace')</dd>
<dt><strong><code>stats_unique</code></strong></dt>
<dd>read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})</dd>
<dt><strong><code>print_impact</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output Dataframe (after outlier treatment),
Analysis Dataframe [attribute, lower_outliers, upper_outliers]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def outlier_detection(idf, list_of_cols=&#39;all&#39;, drop_cols=[], detection_side=&#39;upper&#39;,
                      detection_configs={&#39;pctile_lower&#39;: 0.05, &#39;pctile_upper&#39;: 0.95,
                                         &#39;stdev_lower&#39;: 3.0, &#39;stdev_upper&#39;: 3.0,
                                         &#39;IQR_lower&#39;: 1.5, &#39;IQR_upper&#39;: 1.5,
                                         &#39;min_validation&#39;: 2},
                      treatment=False, treatment_type=&#39;value_replacement&#39;, pre_existing_model=False,
                      model_path=&#34;NA&#34;, output_mode=&#39;replace&#39;, stats_unique={}, print_impact=False):
    &#34;&#34;&#34;

    Args:
      idf: Input Dataframe
      list_of_cols: Numerical Columns (list or string of col names separated by |),\
    all - to include all numerical columns (excluding drop_cols) (Default value = &#39;all&#39;)
      drop_cols: List of columns to be dropped (list or string of col names separated by |) (Default value = [])
      detection_side: upper, lower, both (Default value = &#39;upper&#39;)
      detection_configs: dictionary format - upper &amp; lower bound for each methodology.\
    If an attribute value is less (more) than its derived lower (upper) bound value, \
    it is considered as outlier by a methodology.\
    A attribute value is outliered if it is declared as oultlier by atleast &#39;min_validation&#39; methodologies.\
    (Default value = {&#39;pctile_lower&#39;: 0.05, &#39;pctile_upper&#39;: 0.95, &#39;stdev_lower&#39;: 3.0, &#39;stdev_upper&#39;: 3.0, IQR_lower&#39;: 1.5, &#39;IQR_upper&#39;: 1.5, &#39;min_validation&#39;: 2})
      treatment: if True, cleaning based on treatment_method (Default value = False)
      treatment_type: null_replacement, row_removal, value_replacement (Default value = &#39;value_replacement&#39;)
      pre_existing_model: outlier value for each attribute. True if model files exists already, False Otherwise (Default value = False)
      model_path: If pre_existing_model is True, this argument is path for presaved model file.\
    If pre_existing_model is False, this field can be used for saving the model file.\
    param NA means there is neither pre_existing_model nor there is a need to save one. (Default value = &#34;NA&#34;)
      output_mode: replace or append (Default value = &#39;replace&#39;)
      stats_unique: read_dataset arguments to read pre-saved statistics on unique value count (dictionary format) (Default value = {})
      print_impact:  (Default value = False)

    Returns:
      Output Dataframe (after outlier treatment),\
      Analysis Dataframe [attribute, lower_outliers, upper_outliers]

    &#34;&#34;&#34;

    num_cols = attributeType_segregation(idf)[0]
    if len(num_cols) == 0:
        warnings.warn(&#34;No Outlier Check&#34;)
        odf = idf
        schema = T.StructType([T.StructField(&#39;attribute&#39;, T.StringType(), True),
                               T.StructField(&#39;lower_outliers&#39;, T.StringType(), True),
                               T.StructField(&#39;upper_outliers&#39;, T.StringType(), True)])
        odf_print = spark.sparkContext.emptyRDD().toDF(schema)
        return odf, odf_print
    if list_of_cols == &#39;all&#39;:
        list_of_cols = num_cols
    if isinstance(list_of_cols, str):
        list_of_cols = [x.strip() for x in list_of_cols.split(&#39;|&#39;)]
    if isinstance(drop_cols, str):
        drop_cols = [x.strip() for x in drop_cols.split(&#39;|&#39;)]

    if stats_unique == {}:
        remove_cols = uniqueCount_computation(idf, list_of_cols).where(F.col(&#39;unique_values&#39;) &lt; 2) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()
    else:
        remove_cols = read_dataset(**stats_unique).where(F.col(&#39;unique_values&#39;) &lt; 2) \
            .select(&#39;attribute&#39;).rdd.flatMap(lambda x: x).collect()

    list_of_cols = [e for e in list_of_cols if e not in (drop_cols + remove_cols)]

    if any(x not in num_cols for x in list_of_cols):
        raise TypeError(&#39;Invalid input for Column(s)&#39;)
    if detection_side not in (&#39;upper&#39;, &#39;lower&#39;, &#39;both&#39;):
        raise TypeError(&#39;Invalid input for detection_side&#39;)
    if treatment_type not in (&#39;null_replacement&#39;, &#39;row_removal&#39;, &#39;value_replacement&#39;):
        raise TypeError(&#39;Invalid input for treatment_type&#39;)
    if output_mode not in (&#39;replace&#39;, &#39;append&#39;):
        raise TypeError(&#39;Invalid input for output_mode&#39;)
    if str(treatment).lower() == &#39;true&#39;:
        treatment = True
    elif str(treatment).lower() == &#39;false&#39;:
        treatment = False
    else:
        raise TypeError(&#39;Non-Boolean input for treatment&#39;)
    if str(pre_existing_model).lower() == &#39;true&#39;:
        pre_existing_model = True
    elif str(pre_existing_model).lower() == &#39;false&#39;:
        pre_existing_model = False
    else:
        raise TypeError(&#39;Non-Boolean input for pre_existing_model&#39;)
    for arg in [&#39;pctile_lower&#39;, &#39;pctile_upper&#39;]:
        if arg in detection_configs:
            if (detection_configs[arg] &lt; 0) | (detection_configs[arg] &gt; 1):
                raise TypeError(&#39;Invalid input for &#39; + arg)

    recast_cols = []
    recast_type = []
    for i in list_of_cols:
        if get_dtype(idf, i).startswith(&#39;decimal&#39;):
            idf = idf.withColumn(i, F.col(i).cast(T.DoubleType()))
            recast_cols.append(i)
            recast_type.append(get_dtype(idf, i))

    if pre_existing_model:
        df_model = sqlContext.read.parquet(model_path + &#34;/outlier_numcols&#34;)
        params = []
        for i in list_of_cols:
            mapped_value = df_model.where(F.col(&#39;attribute&#39;) == i).select(&#39;parameters&#39;) \
                .rdd.flatMap(lambda x: x).collect()[0]
            params.append(mapped_value)
    else:
        detection_configs[&#39;pctile_lower&#39;] = detection_configs[&#39;pctile_lower&#39;] or 0.0
        detection_configs[&#39;pctile_upper&#39;] = detection_configs[&#39;pctile_upper&#39;] or 1.0
        pctile_params = idf.approxQuantile(list_of_cols, [detection_configs[&#39;pctile_lower&#39;],
                                                          detection_configs[&#39;pctile_upper&#39;]], 0.01)
        detection_configs[&#39;stdev_lower&#39;] = detection_configs[&#39;stdev_lower&#39;] or detection_configs[&#39;stdev_upper&#39;]
        detection_configs[&#39;stdev_upper&#39;] = detection_configs[&#39;stdev_upper&#39;] or detection_configs[&#39;stdev_lower&#39;]
        stdev_params = []
        for i in list_of_cols:
            mean, stdev = idf.select(F.mean(i), F.stddev(i)).first()
            stdev_params.append(
                [mean - detection_configs[&#39;stdev_lower&#39;] * stdev, mean + detection_configs[&#39;stdev_upper&#39;] * stdev])

        detection_configs[&#39;IQR_lower&#39;] = detection_configs[&#39;IQR_lower&#39;] or detection_configs[&#39;IQR_upper&#39;]
        detection_configs[&#39;IQR_upper&#39;] = detection_configs[&#39;IQR_upper&#39;] or detection_configs[&#39;IQR_lower&#39;]
        quantiles = idf.approxQuantile(list_of_cols, [0.25, 0.75], 0.01)
        IQR_params = [[e[0] - detection_configs[&#39;IQR_lower&#39;] * (e[1] - e[0]),
                       e[1] + detection_configs[&#39;IQR_upper&#39;] * (e[1] - e[0])] for e in quantiles]
        n = detection_configs[&#39;min_validation&#39;]
        params = [[sorted([x[0], y[0], z[0]], reverse=True)[n - 1], sorted([x[1], y[1], z[1]])[n - 1]] for x, y, z in
                  list(zip(pctile_params, stdev_params, IQR_params))]

        # Saving model File if required
        if model_path != &#34;NA&#34;:
            df_model = spark.createDataFrame(zip(list_of_cols, params), schema=[&#39;attribute&#39;, &#39;parameters&#39;])
            df_model.coalesce(1).write.parquet(model_path + &#34;/outlier_numcols&#34;, mode=&#39;overwrite&#39;)

    for i, j in zip(recast_cols, recast_type):
        idf = idf.withColumn(i, F.col(i).cast(j))

    def composite_outlier(*v):
        &#34;&#34;&#34;

        Args:
          *v: 

        Returns:

        &#34;&#34;&#34;
        output = []
        for idx, e in enumerate(v):
            if e is None:
                output.append(None)
                continue
            if detection_side in (&#39;upper&#39;, &#39;both&#39;):
                if e &gt; params[idx][1]:
                    output.append(1)
                    continue
            if detection_side in (&#39;lower&#39;, &#39;both&#39;):
                if e &lt; params[idx][0]:
                    output.append(-1)
                    continue
            output.append(0)
        return output

    f_composite_outlier = F.udf(composite_outlier, T.ArrayType(T.IntegerType()))

    odf = idf.withColumn(&#34;outliered&#34;, f_composite_outlier(*list_of_cols))
    odf.persist()
    output_print = []
    for index, i in enumerate(list_of_cols):
        odf = odf.withColumn(i + &#34;_outliered&#34;, F.col(&#39;outliered&#39;)[index])
        output_print.append(
            [i, odf.where(F.col(i + &#34;_outliered&#34;) == -1).count(), odf.where(F.col(i + &#34;_outliered&#34;) == 1).count()])

        if treatment &amp; (treatment_type in (&#39;value_replacement&#39;, &#39;null_replacement&#39;)):
            replace_vals = {&#39;value_replacement&#39;: [params[index][0], params[index][1]], &#39;null_replacement&#39;: [None, None]}
            odf = odf.withColumn(i + &#34;_outliered&#34;, F.when(F.col(i + &#34;_outliered&#34;) == 1, replace_vals[treatment_type][1]) \
                                 .otherwise(F.when(F.col(i + &#34;_outliered&#34;) == -1, replace_vals[treatment_type][0]) \
                                            .otherwise(F.col(i))))
            if output_mode == &#39;replace&#39;:
                odf = odf.drop(i).withColumnRenamed(i + &#34;_outliered&#34;, i)

    odf = odf.drop(&#34;outliered&#34;)

    if treatment &amp; (treatment_type == &#39;row_removal&#39;):
        for index, i in enumerate(list_of_cols):
            odf = odf.where((F.col(i + &#34;_outliered&#34;) == 0) | (F.col(i + &#34;_outliered&#34;).isNull())).drop(i + &#34;_outliered&#34;)

    if not (treatment):
        odf = idf

    odf_print = spark.createDataFrame(output_print, schema=[&#39;attribute&#39;, &#39;lower_outliers&#39;, &#39;upper_outliers&#39;])
    if print_impact:
        odf_print.show(len(list_of_cols))

    return odf, odf_print</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="com.mw.ds.data_analyzer" href="index.html">com.mw.ds.data_analyzer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.IDness_detection" href="#com.mw.ds.data_analyzer.quality_checker.IDness_detection">IDness_detection</a></code></li>
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.biasedness_detection" href="#com.mw.ds.data_analyzer.quality_checker.biasedness_detection">biasedness_detection</a></code></li>
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.duplicate_detection" href="#com.mw.ds.data_analyzer.quality_checker.duplicate_detection">duplicate_detection</a></code></li>
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.invalidEntries_detection" href="#com.mw.ds.data_analyzer.quality_checker.invalidEntries_detection">invalidEntries_detection</a></code></li>
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.nullColumns_detection" href="#com.mw.ds.data_analyzer.quality_checker.nullColumns_detection">nullColumns_detection</a></code></li>
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.nullRows_detection" href="#com.mw.ds.data_analyzer.quality_checker.nullRows_detection">nullRows_detection</a></code></li>
<li><code><a title="com.mw.ds.data_analyzer.quality_checker.outlier_detection" href="#com.mw.ds.data_analyzer.quality_checker.outlier_detection">outlier_detection</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>